{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edwp-gJFl5G6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "import pylab\n",
        "import seaborn as sns\n",
        "#plt.style.use('seaborn')\n",
        "%matplotlib inline\n",
        "pylab.rcParams['figure.figsize'] = (15, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOvxFPxql5G8"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRHTDwnll5G8"
      },
      "source": [
        "# Кластеризация или обучение без учителя\n",
        "\n",
        "Основное отличие методов обучения без учителя от классификации в том, что разметка для данных отсутствует. Задачи кластеризации возникают при снижении размерности задач, при обработке больших потоков данных (точность обработки при этом теряется, но скорость возрастает), при разработке отдельных моделей для каждого кластера и т.д.\n",
        "\n",
        "Снижение размерности задач имеет ряд полезных свойств:\n",
        "1. Увеличение скорости построения и работы классификатора;\n",
        "2. Снижение зашумленности данных;\n",
        "3. Снижение корреляции признаков;\n",
        "4. Возможность визуализировать многомерные данные на плоскости и др.\n",
        "\n",
        "Вначале познакомимся с методами кластеризации k-Means и агломеративным методом. Затем познакомимся с методами снижения размерности задач PCA и t-SNE. и на последок познакомимся с еще одним методом кластеризации DBSCAN. По каждому рассмотренному методу потребуется решить задание.\n",
        "\n",
        "---\n",
        "\n",
        "Общий принцип кластеризации: требуется разбить множество объектов на группы таким образом, чтобы элементы внутри одной группы (кластера) были похожи друг на друга, а элементы из разных групп отличались."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSXerK4al5G9"
      },
      "source": [
        "## k-Means\n",
        "\n",
        "[Алгоритм к-средних](https://ru.wikipedia.org/wiki/Метод_k-средних) один из самых популярных и простых методов кластеризации. \n",
        "\n",
        "### Подготовка данных\n",
        "\n",
        "Для реализации алгоритма вначале необходимо получить данные с помощью функции [make_blobs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) библиотеки sklearn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB1luYDdl5G-"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, y = make_blobs(n_samples=1000, n_features=5, centers=4, random_state=21)\n",
        "X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm571wvKl5G-"
      },
      "source": [
        "---\n",
        "\n",
        "Нормализуем полученные данные:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HshDS2Yl5G-"
      },
      "outputs": [],
      "source": [
        "X = (X - X.mean(axis=0))/X.std(axis=0)\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKbpXDgDl5G_"
      },
      "source": [
        "---\n",
        "\n",
        "С целью визуализации данных, запишем их в DataFrame и построим графики:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3BFUlaCl5G_"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(X, columns=['x0', 'x1', 'x2', 'x3', 'x4'])\n",
        "df\n",
        "# df['y'] = y\n",
        "# y\n",
        "# sns.pairplot(df, hue=\"y\", height=3)\n",
        "# plt.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyjWUg7vl5HA"
      },
      "source": [
        "Наиболее хорошо кластеры разбиваются по переменным х2 и х4 - именно их мы будем использовать для визуализации на плоскости."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG5q3M6Gl5HA"
      },
      "source": [
        "---\n",
        "\n",
        "Данные готовы. \n",
        "\n",
        "### Обучение модели\n",
        "\n",
        "Импортируем метод [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).\n",
        "\n",
        "Метод к-средних требует указать количество формируемых кластеров. Предположим, что мы не знаем количество кластеров, и укажем требование разбить на 2 кластера. Посмотрим на результаты:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Um-N5DXl5HA"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=21).fit(X)\n",
        "plt.scatter(X[:,2], X[:,4], c=kmeans.labels_)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDltxQgQl5HB"
      },
      "source": [
        "---\n",
        "\n",
        "В результате три кластера были объединены в один."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLJyV1pwl5HB"
      },
      "source": [
        "---\n",
        "\n",
        "Для определения количества кластеров в данных можно воспользоваться \"методом локтя\". Суть метода заключается в том, что строится множество моделей к-средних с возрастающим количеством кластеров. По каждой модели считается метрика $J(C_k)$, которая представляет собой корень из суммы квадратов расстояний от точек до центройдов кластеров, к которым они относятся (***kmeans.inertia_***). Выбирается то количество кластеров, при котором метрика резко уменьшает свое снижение (локоть):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZXYkiGpl5HB"
      },
      "outputs": [],
      "source": [
        "inertia = []\n",
        "for k in range(2, 8):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=21).fit(X)\n",
        "    inertia.append(np.sqrt(kmeans.inertia_))\n",
        "\n",
        "plt.plot(range(2, 8), inertia, marker='s')\n",
        "plt.xlabel('$k$')\n",
        "plt.ylabel('$J(C_k)$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-9SoMrWl5HC"
      },
      "source": [
        "---\n",
        "\n",
        "Методом локтя определили, что требуется 4 кластера:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xShTNPbRl5HC"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=4, random_state=21).fit(X)\n",
        "plt.scatter(X[:,2], X[:,4], c=kmeans.labels_)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_5Lf4vml5HC"
      },
      "source": [
        "---\n",
        "\n",
        "Как оценить качество кластеризации? Если разбиение на кластеры интуитивно понятно или имеются размеченные данные, то можно воспользоваться известными нам Precision, Recall и F-мерой, а в случае сбалансированности кластеров (в каждом кластере примерно одинаковое количество объектов), то можно воспользоваться Accuracy или его аналогом, [индексом Рэнда](https://en.wikipedia.org/wiki/Rand_index) - в библиотеке sklearn [adjusted_rand_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_U9cGRHwl5HC",
        "outputId": "ac186aef-8606-4619-84de-78c76ba44e6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "adjusted_rand_score(y, kmeans.labels_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVPX7yxUl5HD"
      },
      "source": [
        "---\n",
        "\n",
        "Индекс Рэнда равен 1 - это значит, что разбиение на кластеры полностью соответствует классам в данных.\n",
        "\n",
        "---\n",
        "\n",
        "В случае отсутствия размеченных данных и интуитивного понимания разбиения на кластеры, потребуются иные метрики оценки качества кластеризации.\n",
        "\n",
        "[silhouette_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html?highlight=silhouette_score#sklearn.metrics.silhouette_score) позволяет оценить кластеризацию с точки зрения близости объектов к центройду кластера, к которому принадлежат, и удаленность от кластеров, к которым не принадлежат. Недостатком метрики является то, что она хорошо работает только с кластерами круглой формы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmqzNvADl5HD",
        "outputId": "72ff4663-a58b-493f-c9ac-9a464693339e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7886615410756992"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "silhouette_score(X, kmeans.labels_, random_state=21)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRSwdNgNl5HD"
      },
      "source": [
        "---\n",
        "\n",
        "Близость silhouette_score к 1 говорит об идеальном разбиении на кластеры, к -1 - к идеально плохому разбиению.\n",
        "\n",
        "Метрику silhouette_score можно применять для определения количества кластеров вместо метода локтя:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRg-qlopl5HE"
      },
      "outputs": [],
      "source": [
        "silhouette = []\n",
        "for k in range(2, 10):\n",
        "    km = KMeans(n_clusters=k, random_state=21).fit(X)\n",
        "    silhouette.append(silhouette_score(X, km.labels_, random_state=21))\n",
        "\n",
        "plt.plot(range(2, 10), silhouette, marker='s')\n",
        "plt.xlabel('$k$')\n",
        "plt.ylabel('silhouette_score')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr-37yjEl5HE"
      },
      "source": [
        "---\n",
        "\n",
        "Максимальное значение silhouette_score на графике показывает, какое количество кластеров необходимо строить.\n",
        "\n",
        "---\n",
        "\n",
        "Метрика [Калински-Харабаса](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html) рассматривает отношение дисперсии объектов внутри кластера и дисперсии между кластерами. Недостатком метрики является отсутствие граничного значения, поэтому необходимо посчитать метрику в соседних разбиениях по кластерам в $k-1$ и $k+1$. Достоинство метрики - быстрое вычисление:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXX4yHWGl5HE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import calinski_harabasz_score\n",
        "\n",
        "print(calinski_harabasz_score(X, kmeans.labels_))\n",
        "\n",
        "ch = []\n",
        "for k in range(2, 10):\n",
        "    km = KMeans(n_clusters=k, random_state=21).fit(X)\n",
        "    ch.append(calinski_harabasz_score(X, km.labels_))\n",
        "\n",
        "plt.plot(range(2, 10), ch, marker='s')\n",
        "plt.xlabel('$k$')\n",
        "plt.ylabel('calinski_harabasz_score')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOAE21i9l5HE"
      },
      "source": [
        "---\n",
        "\n",
        "За счет скорости, метрика calinski_harabasz_score может быть лучшим способом определить количество кластеров.\n",
        "\n",
        "---\n",
        "\n",
        "Метрика [davies_bouldin_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html) так же оценивает близость объектов внутри кластера и удаленность от других кластеров. Чем ближе значение метрики к нулю - тем лучше."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xL_vk5Jl5HF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import davies_bouldin_score\n",
        "\n",
        "print(davies_bouldin_score(X, kmeans.labels_))\n",
        "\n",
        "db = []\n",
        "for k in range(2, 10):\n",
        "    km = KMeans(n_clusters=k, random_state=21).fit(X)\n",
        "    db.append(davies_bouldin_score(X, km.labels_))\n",
        "\n",
        "plt.plot(range(2, 10), db, marker='s')\n",
        "plt.xlabel('$k$')\n",
        "plt.ylabel('davies_bouldin_score')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JL7sgB3l5HF"
      },
      "source": [
        "---\n",
        "\n",
        "Существуют и другие метрики, но нам для анализа достаточно рассмотренных. Все метрики можно собрать в одну функцию, а их результаты записывать в датафрейм:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89AP2tmpl5HF"
      },
      "outputs": [],
      "source": [
        "def metrics4(X, labels, y):\n",
        "    result = [adjusted_rand_score(y, labels)]\n",
        "    result.append(silhouette_score(X, labels, random_state=21))\n",
        "    result.append(calinski_harabasz_score(X, labels))\n",
        "    result.append(davies_bouldin_score(X, labels))\n",
        "    return result\n",
        "\n",
        "m4 = pd.DataFrame(index = ['adjusted_rand_score', 'silhouette_score', 'calinski_harabasz_score', \n",
        "                           'davies_bouldin_score'])\n",
        "m4['KMeans'] = metrics4(X, kmeans.labels_, y)\n",
        "m4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwv-EzY5l5HF"
      },
      "source": [
        "---\n",
        "\n",
        "Метод к-средних удобен, прост, нагляден и легко интерпретируется. Недостатком метода является то, что он представляет из себя NP-сложную задачу и при больших объемах данных будет медленно работать."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ePcZK1yl5HF"
      },
      "source": [
        "---\n",
        "\n",
        "# Задание 1\n",
        "\n",
        "1. Загрузите данные из файла ***данные для кластеризации.xlsx*** (находится в папке datasets).\n",
        "2. Метки класса из столбца *Office* присвойте вектору *y*, остальные данные для кластеризации присвойте матрице *X*.\n",
        "3. Нормализуйте данные матрицы X.\n",
        "4. Импортируйте метод ***KMeans***.\n",
        "5. Проведите анализ на количество кластеров с помощью метода локтя, а так же на основе метрик *silhouette_score*, *calinski_harabasz_score* и *davies_bouldin_score*. \n",
        "6. Сделайте выводы по результатам анализа.\n",
        "7. Обучите модель *KMeans* с разбиением на два кластера.\n",
        "8. Оцените результат с помощью *classification_report()*, сравнив попадание объектов в кластеры с реальными классами объектов из исходного файла с данными. Имейте в виду, что кластеры - это не классы, поэтому важно то, что одни и те же объекты попали внутрь одной группы, а не то, какой номер группы им присвоен моделью. На протяжении всех заданий мы будем оценивать качество кластеризации, сравнивая кластеры с классификацией данных по признакам: Rain и Office.\n",
        "9. Оцените результат кластеризации с помощью функции *metrics4()* и заполните соответствующий столбец таблицы итогов кластеризации *m4*.\n",
        "10. Сделайте выводы по результатам оценки качества кластеризации.\n",
        "\n",
        "---\n",
        "\n",
        "***Рекомендации:***\n",
        "\n",
        "1. При проведении анализа на количество кластеров выбирайте верхнюю границу диапазона не меньше 100 (для метода локтя 50).\n",
        "2. Не забывайте, что модель может нумеровать кластеры не в том порядке, как в данных."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ЗАДАНИЕ 1\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "i4-fjZ-DD1x5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "import pylab\n",
        "import seaborn as sns\n",
        "#plt.style.use('seaborn')\n",
        "%matplotlib inline\n",
        "pylab.rcParams['figure.figsize'] = (15, 10)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "from google.colab import drive  # если вы выполняете код из среды Google Colab, нужно подключить свой гугл-диск,\n",
        "drive.mount('/content/drive')   # чтобы можно было оттуда считать файл с данными для этого задания\n",
        "df = pd.read_excel('/content/drive/MyDrive/data/данные_для_кластеризации.xlsx') # Создаем датафрейм df из файла\n",
        "df \n",
        "\n"
      ],
      "metadata": {
        "id": "x5gk3-FDDt3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y = df.Rain\n",
        "# X = df.drop('Rain', axis=1)\n",
        "# X\n",
        "\n",
        "# y = df.Office\n",
        "X = df.drop('Office', axis=1)\n",
        "# X = X.drop('Rain', axis=1)\n",
        "X\n",
        "# Z = X.drop('Rain', axis=1)\n",
        "Z = X.drop('weekday_cos', axis=1)\n",
        "Z = Z.drop('Order_bac_cos', axis=1)\n",
        "X\n",
        "# Z\n",
        "\n",
        "Z = (Z - Z.mean(axis=0))/Z.std(axis=0)\n",
        "\n",
        "# Z['Rain'] = X.Rain\n",
        "Z\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=21)\n",
        "# sc = StandardScaler()\n",
        "# sc.fit(X_train)                                  \n",
        "# X_train_std = sc.transform(X_train)              \n",
        "# X_test_std = sc.transform(X_test)\n",
        "\n",
        "# lr = LogisticRegression(random_state=21)\n",
        "# lr.fit(X_train_std, y_train)                     \n",
        "# y_pred = lr.predict(X_test_std)                  \n",
        "\n",
        "# print(classification_report(y_test, y_pred))\n",
        "# X = (X - X.mean(axis=0))/X.std(axis=0)\n",
        "\n",
        "\n",
        "\n",
        "# X = (X - X.mean(axis=0))/X.std(axis=0)\n",
        "# X\n",
        "\n",
        "\n",
        "# from sklearn.cluster import KMeans\n",
        "\n",
        "# kmeans = KMeans(n_clusters=2, random_state=21).fit(X)\n",
        "# plt.scatter(X[:,2], X[:,4], c=kmeans.labels_)\n",
        "# plt.show()\n",
        "\n",
        "# from sklearn.datasets import make_blobs\n",
        "# X, y = make_blobs(n_samples=1000, n_features=5, centers=4, random_state=21)\n",
        "\n",
        "# X = (X - X.mean(axis=0))/X.std(axis=0)\n"
      ],
      "metadata": {
        "id": "sMYriXuUFPKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.DataFrame(X)\n",
        "# df['Office'] = y\n",
        "# # y\n",
        "# sns.pairplot(df, hue=\"Office\", height=3)\n",
        "# plt.plot()\n",
        "\n",
        "df = pd.DataFrame(Z)\n",
        "df['Office'] = y\n",
        "# y\n",
        "# sns.pairplot(df, hue=\"Office\", height=3)\n",
        "# plt.plot()"
      ],
      "metadata": {
        "id": "WTSV4JO-Gdx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X = (X - X.mean(axis=0))/X.std(axis=0)\n",
        "# X\n",
        "# from sklearn.cluster import KMeans\n",
        "\n",
        "# kmeans = KMeans(n_clusters=2, random_state=21).fit(X)\n",
        "# plt.scatter(X[:,2], X[:,4], c=kmeans.labels_)\n",
        "# plt.show()\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=21).fit(Z)\n",
        "\n",
        "# plt.scatter(X['Order_bac_sin'], X['Order_bac_cos'], c=kmeans.labels_)\n",
        "plt.scatter(Z['Order_bac_sin'], Z['Rain'], c=kmeans.labels_)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0TZxQzpJF214"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "adjusted_rand_score(y, kmeans.labels_)"
      ],
      "metadata": {
        "id": "4bMZjRjIHlCr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "165ecd1d-7712-45b3-a8c0-fd7ecd31d6fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6383875030533672"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inertia = []\n",
        "for k in range(1, 4):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=21).fit(Z)\n",
        "    inertia.append(np.sqrt(kmeans.inertia_))\n",
        "\n",
        "plt.plot(range(1, 4), inertia, marker='s')\n",
        "plt.xlabel('$k$')\n",
        "plt.ylabel('$J(C_k)$')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_aw3zA7LICAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "silhouette_score(Z, kmeans.labels_, random_state=21)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9w-6SAcIbXB",
        "outputId": "6200a954-0a39-4f88-892a-ca999b54eeec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2896711295462786"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette = []\n",
        "for k in range(2, 5):\n",
        "    km = KMeans(n_clusters=k, random_state=21).fit(Z)\n",
        "    silhouette.append(silhouette_score(Z, km.labels_, random_state=21))\n",
        "\n",
        "plt.plot(range(2, 5), silhouette, marker='s')\n",
        "plt.xlabel('$k$')\n",
        "plt.ylabel('silhouette_score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "scXaPZ__IybP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import calinski_harabasz_score\n",
        "\n",
        "print(calinski_harabasz_score(Z, kmeans.labels_))\n",
        "\n",
        "ch = []\n",
        "for k in range(2, 5):\n",
        "    km = KMeans(n_clusters=k, random_state=21).fit(Z)\n",
        "    ch.append(calinski_harabasz_score(Z, km.labels_))\n",
        "\n",
        "plt.plot(range(2, 5), ch, marker='s')\n",
        "plt.xlabel('$k$')\n",
        "plt.ylabel('calinski_harabasz_score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c9O5nXWVI5V-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import davies_bouldin_score\n",
        "\n",
        "print(davies_bouldin_score(Z, kmeans.labels_))\n",
        "\n",
        "db = []\n",
        "for k in range(2, 5):\n",
        "    km = KMeans(n_clusters=k, random_state=21).fit(Z)\n",
        "    db.append(davies_bouldin_score(Z, km.labels_))\n",
        "\n",
        "plt.plot(range(2, 5), db, marker='s')\n",
        "plt.xlabel('$k$')\n",
        "plt.ylabel('davies_bouldin_score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9uMvuyukI9V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y = df.Office\n",
        "# X = df.drop('Office', axis=1)\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=21)\n",
        "# sc = StandardScaler()\n",
        "# sc.fit(X_train)                                  \n",
        "# X_train_std = sc.transform(X_train)              \n",
        "# X_test_std = sc.transform(X_test)\n",
        "\n",
        "# lr = LogisticRegression(random_state=21)\n",
        "# lr.fit(X_train_std, y_train)                     \n",
        "# y_pred = lr.predict(X_test_std)  \n",
        "\n",
        "# from sklearn.datasets import make_blobs\n",
        "# # X\n",
        "# X, y = make_blobs(n_samples=1000, n_features=5, centers=4, random_state=21)\n",
        "# X, y\n",
        "# X\n",
        "# y\n",
        "# X\n",
        "# X = (X - X.mean(axis=0))/X.std(axis=0)\n",
        "# X\n",
        "# kmeans = KMeans(n_clusters=2, random_state=21).fit(X)\n",
        "# plt.scatter(X[:,2], X[:,4], c=kmeans.labels_)\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P1F0wCSJNi9",
        "outputId": "a9d66cf4-d227-4cd0-8de4-74c9ce30f8ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-8.13662364, -4.81444713,  3.9359552 , -8.21735083, -4.63344305],\n",
              "       [-6.42268911, -6.20108463,  3.49246328, -8.62177893, -6.63212885],\n",
              "       [-9.49480613, -2.71346591,  2.9051504 , -1.09377871,  1.7888751 ],\n",
              "       ...,\n",
              "       [-9.31503967, -4.80203821,  5.05315616, -4.47198345,  2.63358165],\n",
              "       [-8.83400659, -4.23418288,  2.84614297, -2.73972074,  2.5464589 ],\n",
              "       [-9.05640116,  6.35742895, -7.11944391, -7.10158446, -1.34642424]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.cluster import hierarchy\n",
        "# X\n",
        "# X2 = X.drop('weekday_sin', axis=1)\n",
        "# X2 = X2.drop('weekday_cos', axis=1)\n",
        "# X2 = X2.drop('Delivery_time_norm', axis=1)\n",
        "# X2 = X2.drop('Rain', axis=1)\n",
        "# X2 = X2.drop('Office', axis=1)\n",
        "# X2\n",
        "model_2 = AgglomerativeClustering(n_clusters=3).fit(Z)\n",
        "print(classification_report(y, model_2.labels_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oZwchonJ4BN",
        "outputId": "d8dd9882-06ba-444f-c64e-1977821e2635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.82      0.86      9389\n",
            "           1       0.73      0.38      0.50     11402\n",
            "           2       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.58     20791\n",
            "   macro avg       0.55      0.40      0.45     20791\n",
            "weighted avg       0.81      0.58      0.66     20791\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "# y\n",
        "def metrics4(Z, labels, y):\n",
        "    result = [adjusted_rand_score(y, labels)]\n",
        "    result.append(silhouette_score(Z, labels, random_state=21))\n",
        "    result.append(calinski_harabasz_score(Z, labels))\n",
        "    result.append(davies_bouldin_score(Z, labels))\n",
        "    return result\n",
        "\n",
        "m4 = pd.DataFrame(index = ['adjusted_rand_score', 'silhouette_score', 'calinski_harabasz_score', \n",
        "                           'davies_bouldin_score'])\n",
        "m4['KMeans'] = metrics4(Z, model_2.labels_, y)\n",
        "m4"
      ],
      "metadata": {
        "id": "jwjjEvQ9LBAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод:2/3 clasters\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xbpASOgfLfZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# повторим некоторые функции, которые пригодятся вам при решении задания\n",
        "\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    \"\"\"Функция, которая строит график ошибок классификации TP, FP, TN, FN. Она нам понадобится\n",
        "    для оценки качества кластеризации при сравнении кластеров с классами\"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    thresh = cm.max() / 2.\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        cm = np.around(cm, decimals=3)\n",
        "        thresh = 0.5   \n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('Class label')\n",
        "    plt.xlabel('Cluster label')\n",
        "    \n",
        "def plot_dendrogram(model, **kwargs):\n",
        "    \"\"\"Функция, которая принимает модель агломеративной кластеризации и строит\n",
        "    дендрограмму исследуемых данных\"\"\"\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "\n",
        "    linkage_matrix = np.column_stack([model.children_, model.distances_, counts]).astype(float)\n",
        "    hierarchy.dendrogram(linkage_matrix, **kwargs)\n",
        "\n",
        "def metrics4(X, labels, y):\n",
        "    \"\"\"Функция, которая вычисляет различные метрики, которые мы будем использовать\n",
        "    для оценки качества кластеризации. Здесь у - это колонка с реальными метками\n",
        "    классов датасета\"\"\"\n",
        "    result = [adjusted_rand_score(y, labels)]\n",
        "    result.append(silhouette_score(X, labels, random_state=21))\n",
        "    result.append(calinski_harabasz_score(X, labels))\n",
        "    result.append(davies_bouldin_score(X, labels))\n",
        "    return result\n",
        "\n",
        "m4 = pd.DataFrame(index = ['adjusted_rand_score', 'silhouette_score', 'calinski_harabasz_score', \n",
        "                           'davies_bouldin_score'])"
      ],
      "metadata": {
        "id": "33Csdlen_M5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHnKskE_l5HF"
      },
      "source": [
        "---\n",
        "\n",
        "## Агломеративный метод\n",
        "\n",
        "Агломеративный метод - один из методов иерархической кластеризации. Иерархическая кластеризация позволяет получить не просто разбиение на кластеры, а целую иерархию вложенных кластеров. Агломеративный метод изначально считает каждый объект отдельным классом, затем на каждом шаге он начинает объединять попарно ближайшие классы. Алгоритм прекращает работу, когда все объекты объединены в один класс.\n",
        "\n",
        "Достоинства метода:\n",
        "1. В результате имеем иерархию возможных разбиений на кластеры.\n",
        "2. Удобная визуализация результатов в виде дендрограммы.\n",
        "\n",
        "Недостатки метода:\n",
        "1. Требует большие объемы памяти компьютера.\n",
        "2. Медленно работает при больших объемах данных.\n",
        "\n",
        "---\n",
        "\n",
        "Ипортируем агломеративный методод [AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) и [hierarchy](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html) от куда нам понадобится функция [dendrogram](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram) для визуализации дендрограммы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gRabCHGl5HF"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.cluster import hierarchy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKFAObKol5HG"
      },
      "source": [
        "---\n",
        "\n",
        "Напишем функцию подготовки данных и визуализации дендрограммы и обучим модель кластеризации агломеротивным методом:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQ-jQMIql5HG"
      },
      "outputs": [],
      "source": [
        "def plot_dendrogram(model, **kwargs):\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "    linkage_matrix = np.column_stack([model.children_, model.distances_, counts]).astype(float)\n",
        "    hierarchy.dendrogram(linkage_matrix, **kwargs)\n",
        "    \n",
        "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None).fit(Z)\n",
        "plot_dendrogram(model, truncate_mode='level', p=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cghiXQuWl5HG"
      },
      "source": [
        "---\n",
        "\n",
        "Количество кластеров в данных можно определить на основе дендрограммы - большие расстояния между кластерами и их объединением (ось ординат) свидетельствуют о различных кластерах. Анализируя полученную дендрограмму можно сделать вывод о наличии четырех кластеров.\n",
        "\n",
        "Обучим модель непосредственно для 4 кластеров и оценим качество кластеризации:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuEftG0jl5HG"
      },
      "outputs": [],
      "source": [
        "model_2 = AgglomerativeClustering(n_clusters=4).fit(X)\n",
        "print(classification_report(y, model_2.labels_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jORZePezl5HG"
      },
      "source": [
        "---\n",
        "\n",
        "Анализ показывает, что мы угадали только 50% объектов, при этом в первом и четвертом классе мы не угадали верно ни одного объекта. \n",
        "\n",
        "Посмотрим визуализацию кластеризации:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "wE1lPHF0l5HG"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X[:,2], X[:,4], c=model_2.labels_)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB4deP0pl5HH"
      },
      "source": [
        "---\n",
        "\n",
        "Визуальный анализ показывает, что все кластеры были верно предсказаны. Следовательно проблема в том, что метод просто нумерует кластеры не в той последовательности, что заложена изначально. Проверим это:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UuAIQrkl5HH"
      },
      "outputs": [],
      "source": [
        "for j in range(model_2.labels_.max()+1):\n",
        "    print('метка класса =', j, '   метка кластера =', model_2.labels_[y==j].mean(), \n",
        "          '   стандартное отклонение меток кластера =', model_2.labels_[y==j].std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36Jg8pPsl5HH"
      },
      "source": [
        "---\n",
        "\n",
        "Получается, что классу с меткой 0 соответствует кластер с меткой 3 и наоборот. Для дальнейшей оценки качества кластеризаци создадим вектор меток классов y_ac, соответствующий результатам кластеризации:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6pPxzb5l5HH"
      },
      "outputs": [],
      "source": [
        "y_ac = np.full(y.shape, fill_value=np.nan)\n",
        "for j in range(4):\n",
        "     y_ac[y == j] = round(model_2.labels_[y==j].mean(), 0)\n",
        "print(classification_report(y_ac, model_2.labels_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJwEVstEl5HH"
      },
      "source": [
        "---\n",
        "\n",
        "100% объектов классов угаданы при кластеризации.\n",
        "\n",
        "Подсчитаем и запишем в таблицу метрики качества кластеризации:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_S0aPPQl5HH"
      },
      "outputs": [],
      "source": [
        "m4['AgglomerativeClustering'] = metrics4(X, model_2.labels_, y_ac)\n",
        "m4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh-dKuKnl5HH"
      },
      "source": [
        "---\n",
        "\n",
        "# Задание 2\n",
        "\n",
        "1. Обучите общую модель агломеративной кластеризации.\n",
        "2. Сделайте вывод о количестве кластеров в данных на основе дендрограммы.\n",
        "3. Обучите модель агломеративной кластеризации для двух кластеров.\n",
        "4. Оцените качество кластеризации и запишите результаты в таблицу.\n",
        "5. Сделайте выводы о результатах кластеризации.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.cluster import hierarchy\n",
        "\n",
        "def plot_dendrogram(model, **kwargs):\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "    linkage_matrix = np.column_stack([model.children_, model.distances_, counts]).astype(float)\n",
        "    hierarchy.dendrogram(linkage_matrix, **kwargs)\n",
        "    \n",
        "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None).fit(X)\n",
        "plot_dendrogram(model, truncate_mode='level', p=3)"
      ],
      "metadata": {
        "id": "DkjRvf_pMPK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = AgglomerativeClustering(n_clusters=2).fit(X)\n",
        "print(classification_report(y, model_2.labels_))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0Nca12lQb7Y",
        "outputId": "89939ba9-f4bf-4313-8b09-2ee8db4d325c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.91      0.83      9389\n",
            "           1       0.91      0.76      0.83     11402\n",
            "\n",
            "    accuracy                           0.83     20791\n",
            "   macro avg       0.84      0.84      0.83     20791\n",
            "weighted avg       0.84      0.83      0.83     20791\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(model_2.labels_.max()+1):\n",
        "    print('метка класса =', j, '   метка кластера =', model_2.labels_[y==j].mean(), \n",
        "          '   стандартное отклонение меток кластера =', model_2.labels_[y==j].std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNnFWQpzQrwk",
        "outputId": "8920c6b7-c7b6-4e9f-ae83-be7d9eccd8ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "метка класса = 0    метка кластера = 0.08744275215677921    стандартное отклонение меток кластера = 0.2824827733721603\n",
            "метка класса = 1    метка кластера = 0.7633748465181547    стандартное отклонение меток кластера = 0.4250102236670766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_ac = np.full(y.shape, fill_value=np.nan)\n",
        "for j in range(4):\n",
        "     y_ac[y == j] = round(model_2.labels_[y==j].mean(), 0)\n",
        "print(classification_report(y_ac, model_2.labels_))"
      ],
      "metadata": {
        "id": "lF-BgyatQrDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m4['AgglomerativeClustering'] = metrics4(X, model_2.labels_, y_ac)\n",
        "m4"
      ],
      "metadata": {
        "id": "IEjmhVINQ1Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhuWaE-kl5HI"
      },
      "source": [
        "## Метод главных компонент\n",
        "\n",
        "Метод главных компонент [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) позволяет с помощью линейных преобразований исходных признаков перейти в новое признаковое пространство. В результате преобразований появляется возможность снизить количество признаков при минимальной потере дисперсии. Количество главных компонент опеределяется исследователем на основании требования к сохраненной дисперсии. Обычно ориентируются на 90% дисперсии, но, исходя из конкретики задачи, требование к сохраненной дисперии может быть и больше и меньше. Например, при сильном шуме в данных, требование к сохраненной дисперсии может быть значительно ниже 90%. Порог сохраненной дисперсии может определяться эксперементально, на основе качества решения итоговой задачи.\n",
        "\n",
        "Достоинства метода главных компонент:\n",
        "1. Главные компоненты ортогональны друг другу, что позволяет решить проблему мультиколлинеарности признаков.\n",
        "2. В случае большого количества исходных признаков, позволяет снизить вычислительную нагрузку в задачах классификации и регресии с незначительной потерей качества результата.\n",
        "3. За счет отбрасывания части главных компонент, позволяет снизить шум в данных, что снижает эффект переобучения моделей.\n",
        "\n",
        "Недостатки метода:\n",
        "1. Плохо работает в случае низкой корреляции среди исходных признаков.\n",
        "2. Плохо работает для визуализации многомерных пространств признаков на плоскости."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k4w0sBkl5HI"
      },
      "source": [
        "---\n",
        "\n",
        "Импортируем метод PCA и обучим модель для 5 главных компонент:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBOOXsrCl5HI"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=5, random_state=21)\n",
        "X_centered = X - X.mean(axis=0)\n",
        "pca.fit(X_centered)\n",
        "X_pca = pca.transform(X_centered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyj4NLyGl5HI"
      },
      "source": [
        "---\n",
        "\n",
        "Визуализируем накопленную сохраненную димперсию:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8700CSzdl5HJ"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(1,6), np.cumsum(pca.explained_variance_ratio_), color='k', lw=2)\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Total explained variance')\n",
        "plt.xlim(1, 6)\n",
        "plt.yticks(np.arange(0, 1.1, 0.1))\n",
        "plt.axhline(0.9, c='r')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAXetzrtl5HR"
      },
      "outputs": [],
      "source": [
        "print('Первые две главных компоненты сохраняют', str(round(100*pca.explained_variance_ratio_[:2].sum(),2))+'%', \n",
        "      'дисперсии, три компоненты -', str(round(100*pca.explained_variance_ratio_[:3].sum(),2))+'%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2CXLy7Fl5HS"
      },
      "source": [
        "---\n",
        "\n",
        "С целью снижения размерности данных имеет смысл оставлять три главных компоненты. Однако, в нашей задаче и две главных компоненты выполняют свою задачу по разделению кластеров:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yn8YsY0tl5HS"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2, random_state=21)\n",
        "X_centered = X - X.mean(axis=0)\n",
        "pca.fit(X_centered)\n",
        "X_pca = pca.transform(X_centered)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnMU3hNCl5HS"
      },
      "source": [
        "---\n",
        "\n",
        "Однако два кластера оказались достаточно близко друг к другу. При подготовке этого ноутбука, данные многократно обновлялись, и на многих из них кластеры пересекались."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6WfeTzAl5HS"
      },
      "source": [
        "---\n",
        "\n",
        "# Задание 3\n",
        "\n",
        "1. Обучить модель для 6 главных компонент.\n",
        "2. Визуализировать сохраненную дисперсию.\n",
        "3. Определить количество главных компонент, сохраняющих не менее 90% дисперсии.\n",
        "4. Обучить модель для двух главных компонент и визуализировать данные.\n",
        "5. Сделать выводы об эффективности применения метода главных компонент к решаемой задаче. По какому признаку можно разделить выборку на кластеры?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ4PeGMIl5HS"
      },
      "source": [
        "---\n",
        "\n",
        "## t-SNE\n",
        "\n",
        "[t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) - еще один метод снижения размерности. В отличие от метода главных компонент, t-SNE сжимает данные таким образом, чтобы сохранить их структуру. При этом полученное расстояние в сжатом признаковом в пространстве скорее всего никак не будет соотносится с расстояниями в исходном признаковом в пространстве. Метод пытается перенести окрестность каждой точки из исходного пространства в сжатое. \n",
        "\n",
        "Достоинства метода:\n",
        "1. Метод прост в применении.\n",
        "2. Работает с большими объемами данных.\n",
        "3. Эффективен при визуализации на плоскости многомерных данных.\n",
        "\n",
        "Недостатки метода:\n",
        "1. Полученные результаты практически не интерпретируются.\n",
        "2. Может найти структуру в абсолютном шуме."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ9uW-UNl5HT"
      },
      "source": [
        "---\n",
        "\n",
        "Импортируем метод t-SNE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ikwq04z7l5HT"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPLfXCnBl5HT"
      },
      "source": [
        "Обучим модели для двух компонент, с гиперпараметром ***perplexity*** равным 10 и 100 и визуализируем результаты:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "T0efxKnZl5HT"
      },
      "outputs": [],
      "source": [
        "tsne = TSNE(n_components=2, perplexity=10, random_state=21)\n",
        "tsne_representation = tsne.fit_transform(X)\n",
        "plt.scatter(tsne_representation[:, 0], tsne_representation[:, 1])\n",
        "plt.title('perplexity = 10')\n",
        "plt.show()\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=100, random_state=21)\n",
        "tsne_representation = tsne.fit_transform(X)\n",
        "plt.scatter(tsne_representation[:, 0], tsne_representation[:, 1])\n",
        "plt.title('perplexity = 100')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX8KrMktl5HU"
      },
      "source": [
        "---\n",
        "\n",
        "Увеличение параметра perplexity приводит к уменьшению расстояния между точками.\n",
        "\n",
        "Визуализируем принадлежность к кластерам при двух компонентах и perplexity=25:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM9tn9z8l5HU"
      },
      "outputs": [],
      "source": [
        "tsne = TSNE(n_components=2, perplexity=25, random_state=21)\n",
        "tsne_representation = tsne.fit_transform(X)\n",
        "plt.scatter(tsne_representation[:, 0], tsne_representation[:, 1], c=y)\n",
        "plt.title('perplexity = 25')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJKC7RCCl5HU"
      },
      "source": [
        "---\n",
        "\n",
        "# Задание 4\n",
        "\n",
        "Визуализируйте данные с помощью t-SNE при различных значениях perplexity. По какому признаку можно разделить данные на кластеры при достаточно большом значении perplexity?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFMtPs6Kl5HU"
      },
      "source": [
        "---\n",
        "\n",
        "## DBSCAN\n",
        "\n",
        "Еще один метод кластеризации - [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html).\n",
        "\n",
        "Достоинства метода:\n",
        "\n",
        "1. Самостоятельно определяет количество кластеров в зависимости от параметра eps.\n",
        "2. Кроме кластеров определяет шум.\n",
        "3. Работает с кластерами разной формы.\n",
        "\n",
        "Недостатки метода:\n",
        "\n",
        "1. С помощью метода невозможно проверить гипотезу о количестве кластеров, т.к. метод определяет их самостоятельно.\n",
        "2. Необходим поиск параметра eps.\n",
        "3. Не работает, если кластеры разной плотности.\n",
        "\n",
        "Импортируем метод:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sEgRxMul5HU"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89_OX-Rol5HU"
      },
      "source": [
        "---\n",
        "\n",
        "Для отображения результатов на плоскости снизим размерность задачи с помощью t-SNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aItc6qb4l5HU"
      },
      "outputs": [],
      "source": [
        "tsne = TSNE(n_components=2, perplexity=15, random_state=21)\n",
        "tsne_X = tsne.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CefPE80Il5HV"
      },
      "source": [
        "---\n",
        "\n",
        "Обучим модель с параметрами по умолчанию и напишем функцию визуализации результатов кластеризации:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvQZBdKvl5HV"
      },
      "outputs": [],
      "source": [
        "dbscan = DBSCAN(eps=0.5)\n",
        "dbscan.fit(X)\n",
        "\n",
        "def viz_dbscan(labels, x2d):\n",
        "    clstr = []\n",
        "    k = labels.min()\n",
        "    if k == -1:\n",
        "        title = ['DBSCAN нашел ', ' кластера и шум']\n",
        "    else:\n",
        "        title = ['DBSCAN нашел ', ' кластера']\n",
        "    for j in range(k, labels.max()+1):\n",
        "        plt.scatter(x2d[labels==j, 0], x2d[labels==j, 1])\n",
        "        if j == -1:\n",
        "            clstr = ['Шум']\n",
        "        else:\n",
        "            clstr.append('Кластер '+str(j+1))\n",
        "    plt.legend(clstr)\n",
        "    plt.title(title[0] + str(len(clstr)+k) + title[1])\n",
        "    plt.show()\n",
        "    \n",
        "viz_dbscan(dbscan.labels_, tsne_X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xPOOQKol5HV"
      },
      "source": [
        "---\n",
        "\n",
        "Кластеры определены верно, но найден шум, который в принципе необходимо удалить. Попробуем изменить параметр ***eps*** модели. В этом нам поможет [NearestNeighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html). Установим количество ближайших соседей *n_neighbors=25*, это означает что в кластере должно быть не менее 25 объектов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8NyOt7Hl5HV"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import NearestNeighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Are7380Yl5HV"
      },
      "outputs": [],
      "source": [
        "model_NN = NearestNeighbors(n_neighbors=25)\n",
        "model_NN.fit(X)\n",
        "dist, _ = model_NN.kneighbors(X, n_neighbors=25, return_distance=True)\n",
        "dist = np.sort(dist[:,-1])\n",
        "plt.plot(dist)\n",
        "plt.plot([0,1000], [0.8, 0.8], 'r')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac17VzjNl5HV"
      },
      "source": [
        "---\n",
        "\n",
        "Параметр ***eps*** по оси ординат необходимо выбрать так, чтобы после пересечения прямой с графиком, график шел вертикально вверх. Построим модель с eps=0.8:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVP9VFril5HV"
      },
      "outputs": [],
      "source": [
        "dbscan = DBSCAN(eps=0.8)\n",
        "dbscan.fit(X)\n",
        "viz_dbscan(dbscan.labels_, tsne_X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRkPQQWal5HV"
      },
      "source": [
        "---\n",
        "\n",
        "DBSCAN верно определил кластеры и не нашел шума.\n",
        "\n",
        "---\n",
        "\n",
        "Оценим качество кластеризации:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXaHRqN4l5HV"
      },
      "outputs": [],
      "source": [
        "for j in range(dbscan.labels_.max()+1):\n",
        "    print('метка класса =', j, '   метка кластера =', dbscan.labels_[y==j].mean(), \n",
        "          '   стандартное отклонение меток кластера =', dbscan.labels_[y==j].std())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VU5hwGiql5HV"
      },
      "outputs": [],
      "source": [
        "y_ac = np.full(y.shape, fill_value=np.nan)\n",
        "for j in range(4):\n",
        "     y_ac[y == j] = int(dbscan.labels_[y==j].mean())\n",
        "print(classification_report(y_ac, dbscan.labels_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOdyALrel5HW"
      },
      "outputs": [],
      "source": [
        "m4['DBSCAN'] = metrics4(X, dbscan.labels_, y_ac)\n",
        "m4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-5mp_OLl5HW"
      },
      "source": [
        "---\n",
        "\n",
        "Все методы показали одинаковые значения метрик качества кластеризации, в силу того, что продемонстрировали одинаковые результаты.\n",
        "\n",
        "---\n",
        "\n",
        "Можно посмотреть, при каких значениях eps сколько кластеров строит DBSCAN. Например, следующий код строит зависимость количества кластеров от eps, изменяющегося от 0.1 до 0.99:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDnqvbTfl5HW"
      },
      "outputs": [],
      "source": [
        "epsilon = []\n",
        "n_claster = []\n",
        "for j in range(10, 100, 1):\n",
        "    dbscan = DBSCAN(eps=j/100)\n",
        "    dbscan.fit(X)\n",
        "    epsilon.append(j/100)\n",
        "    n_claster.append(dbscan.labels_.max()+1)\n",
        "plt.plot(epsilon, n_claster, marker='s')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxNEHZjbl5HW"
      },
      "source": [
        "---\n",
        "\n",
        "И можно посмотреть, как DBSCAN разбивает наши данные на 19 кластеров (eps=0.18):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWqxzH6Ql5HW"
      },
      "outputs": [],
      "source": [
        "dbscan = DBSCAN(eps=0.18)\n",
        "dbscan.fit(X)\n",
        "viz_dbscan(dbscan.labels_, tsne_X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gEeu01Jl5HX"
      },
      "source": [
        "---\n",
        "\n",
        "Для оценки качества полученного разбиения не обязательно применять метрики, достаточно взглянуть на процент данных, определенных как шум:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja6kJcKyl5HX",
        "outputId": "02045a18-0069-485f-92da-d30b806da913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DBSCAN определил 70.8% данных как шум\n"
          ]
        }
      ],
      "source": [
        "print('DBSCAN определил', str(len(dbscan.labels_[dbscan.labels_==-1])/10) +'%', 'данных как шум')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZCmp8hll5HX"
      },
      "source": [
        "---\n",
        "\n",
        "# Задание 5\n",
        "\n",
        "1. Заголовок последнего графика содержит ошибку \"19 кластера\". Необходимо написать функцию ***add_a_ov(n, word)***, которая поможет исправить ошибку. Функция обладает следующими характеристиками:\n",
        "    - Функция получает на вход два параметра n и word:\n",
        "        - n - числительное в виде целого числа; \n",
        "        - word - существительное мужского рода в единственном числе в именительном падеже.\n",
        "    - Функция возвращает измененное word в соответствии с числительным n.\n",
        "    - Примеры выполнения функции:\n",
        "        - *add_a_ov(1, 'кластер')* вернет 'кластер';\n",
        "        - *add_a_ov(2, 'кластер')* вернет 'кластера';\n",
        "        - *add_a_ov(5, 'кластер')* вернет 'кластеров'.\n",
        "        \n",
        "        \n",
        "2. Внесите изменения в функцию ***viz_dbscan()*** с учетом реализации функции ***add_a_ov(n, word)***.\n",
        "3. Обучите модель DBSCAN с параметрами по умолчанию и визуализируйте результат.\n",
        "4. Проведите поиск гиперпараметра ***eps***.\n",
        "5. Проведите визуализацию зависимости количества кластеров от параметра ***eps***.\n",
        "6. Обучите модель DBSCAN с таким параметром ***eps***, чтобы получить разбиение на два кластера.\n",
        "7. Визуализируйте результат.\n",
        "8. Проведите анализ соответствия полученных кластеров классам.\n",
        "9. Оцените качество кластеризации и запишите результаты в таблицу.\n",
        "10. Сделайте общий вывод по результатам кластеризации различными методами."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_a_ov(n, word):\n",
        "  _ret = \"\"\n",
        "  if n % 100 > 10 and n % 100 < 20:\n",
        "    return word + \"ов\"\n",
        "  if n % 10 == 1:\n",
        "    return word\n",
        "  elif n % 10 > 1 and n % 10 < 5:\n",
        "    return word + \"а\"\n",
        "  else:\n",
        "    return word + \"ов\"\n",
        "\n",
        "dbscan = DBSCAN(eps=0.5)\n",
        "dbscan.fit(X)\n",
        "\n",
        "def viz_dbscan(labels, x2d):\n",
        "    clstr = []\n",
        "    k = labels.min()\n",
        "    print(k)\n",
        "    if k == -1:\n",
        "        title = ['DBSCAN нашел ', ' ' + add_a_ov(labels.max() + 1, 'кластер') + ' и шум']\n",
        "    else:\n",
        "        title = ['DBSCAN нашел ', ' ' + add_a_ov(labels.max() + 1, 'кластер')]\n",
        "    for j in range(k, labels.max()+1):\n",
        "        plt.scatter(x2d[labels==j, 0], x2d[labels==j, 1])\n",
        "        if j == -1:\n",
        "            clstr = ['Шум']\n",
        "        else:\n",
        "            clstr.append('Кластер '+str(j+1))\n",
        "    plt.legend(clstr)\n",
        "    plt.title(title[0] + str(len(clstr)+k) + title[1])\n",
        "    plt.show()\n",
        "    \n",
        "viz_dbscan(dbscan.labels_, tsne_X)"
      ],
      "metadata": {
        "id": "g6uWCdm-TM_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuhSlUHpl5HX"
      },
      "outputs": [],
      "source": [
        "# код проверки правильности функции add_a_ov. Запустите его, чтобы узнать, правильно ли описана функция\n",
        "\n",
        "def test(n, word):\n",
        "    n = n%100\n",
        "    if n%10 in [2,3,4] and n//10 != 1:\n",
        "        result = word + 'а'\n",
        "    elif n%10 == 1 and n != 11:\n",
        "        result = word\n",
        "    else:\n",
        "        result = word + 'ов'\n",
        "    return result\n",
        "\n",
        "\n",
        "for j in [i for i in range(31)]+[101,102,105,111,112,1001,1002,1005,1011,1012,1021,1022]:\n",
        "    if test(j, 'слон') != add_a_ov(j, 'слон'):\n",
        "        break\n",
        "if j < 1022:\n",
        "    print(j, add_a_ov(j, 'слон'))\n",
        "    if j < 10:\n",
        "        print('Функция содержит критичные ошибки')\n",
        "    elif j < 20:\n",
        "        print('Функция содержит критичные ошибки. Не верно обрабатываются числительные от 11 до 14')\n",
        "    elif j < 100:\n",
        "        print('Функция содержит критичные ошибки. Не верно обрабатываются числительные от 20 до 100')\n",
        "    elif j < 1000:\n",
        "        print('Функция содержит не критичные для задачи ошибки.', \n",
        "              'Не верно обрабатываются числительные свыше 100')\n",
        "    elif j >= 1000:\n",
        "        print('Функция содержит не критичные для задачи ошибки.', \n",
        "              'Не верно обрабатываются числительные от 1000 и выше')\n",
        "else:\n",
        "    print('Функция работает нормально')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=15, random_state=21)\n",
        "tsne_X = tsne.fit_transform(X)\n",
        "\n",
        "dbscan = DBSCAN(eps=0.5)\n",
        "dbscan.fit(X)\n",
        "\n",
        "def viz_dbscan(labels, x2d):\n",
        "    clstr = []\n",
        "    k = labels.min()\n",
        "    if k == -1:\n",
        "        title = ['DBSCAN нашел ', ' кластера и шум']\n",
        "    else:\n",
        "        title = ['DBSCAN нашел ', ' кластера']\n",
        "    for j in range(k, labels.max()+1):\n",
        "        plt.scatter(x2d[labels==j, 0], x2d[labels==j, 1])\n",
        "        if j == -1:\n",
        "            clstr = ['Шум']\n",
        "        else:\n",
        "            clstr.append('Кластер '+str(j+1))\n",
        "    plt.legend(clstr)\n",
        "    plt.title(title[0] + str(len(clstr)+k) + title[1])\n",
        "    plt.show()\n",
        "    \n",
        "viz_dbscan(dbscan.labels_, tsne_X)"
      ],
      "metadata": {
        "id": "uZdzen6oU9lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "model_NN = NearestNeighbors(n_neighbors=25)\n",
        "model_NN.fit(X)\n",
        "dist, _ = model_NN.kneighbors(X, n_neighbors=25, return_distance=True)\n",
        "dist = np.sort(dist[:,-1])\n",
        "plt.plot(dist)\n",
        "plt.plot([0,1000], [0.8, 0.8], 'r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cXtThbCqVnU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan = DBSCAN(eps=0.8)\n",
        "dbscan.fit(X)\n",
        "viz_dbscan(dbscan.labels_, tsne_X)"
      ],
      "metadata": {
        "id": "Jt5R7hUAVvtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(dbscan.labels_.max()+1):\n",
        "    print('метка класса =', j, '   метка кластера =', dbscan.labels_[y==j].mean(), \n",
        "          '   стандартное отклонение меток кластера =', dbscan.labels_[y==j].std())"
      ],
      "metadata": {
        "id": "T-4wplnBV16m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_ac = np.full(y.shape, fill_value=np.nan)\n",
        "for j in range(4):\n",
        "     y_ac[y == j] = int(dbscan.labels_[y==j].mean())\n",
        "print(classification_report(y_ac, dbscan.labels_))"
      ],
      "metadata": {
        "id": "YdP9eo7oV4cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m4['DBSCAN'] = metrics4(X, dbscan.labels_, y_ac)\n",
        "m4"
      ],
      "metadata": {
        "id": "5U045ULyV7BO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = []\n",
        "n_claster = []\n",
        "for j in range(10, 100, 1):\n",
        "    dbscan = DBSCAN(eps=j/100)\n",
        "    dbscan.fit(X)\n",
        "    epsilon.append(j/100)\n",
        "    n_claster.append(dbscan.labels_.max()+1)\n",
        "plt.plot(epsilon, n_claster, marker='s')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VWzieZi5WByU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan = DBSCAN(eps=0.165)\n",
        "dbscan.fit(X)\n",
        "viz_dbscan(dbscan.labels_, tsne_X)"
      ],
      "metadata": {
        "id": "fmLjp5okWJWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('DBSCAN определил', str(len(dbscan.labels_[dbscan.labels_==-1])/10) +'%', 'данных как шум')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFy6o3j_WXxj",
        "outputId": "594ac1c2-200b-4c8d-bf15-1d3a4ce4d1a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DBSCAN определил 98.9% данных как шум\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(dbscan.labels_.max()+1):\n",
        "    print('метка класса =', j, '   метка кластера =', dbscan.labels_[y==j].mean(), \n",
        "          '   стандартное отклонение меток кластера =', dbscan.labels_[y==j].std())"
      ],
      "metadata": {
        "id": "9FVk70_BWv7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_ac = np.full(y.shape, fill_value=np.nan)\n",
        "for j in range(4):\n",
        "     y_ac[y == j] = int(dbscan.labels_[y==j].mean())\n",
        "print(classification_report(y_ac, dbscan.labels_))"
      ],
      "metadata": {
        "id": "CXACOQiLW7uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m4['DBSCAN'] = metrics4(X, dbscan.labels_, y_ac)\n",
        "m4"
      ],
      "metadata": {
        "id": "IV79RCpGXAS4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "d04_desk.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}