{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "d06_desc.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Решение задания №1\n",
        "\n",
        "!pip install pymorphy2 nltk\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "!pip install gensim\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount(\"/content/drive/\", force_remount=True)"
      ],
      "metadata": {
        "id": "XBzvsrWMMabZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "columns = ['id', 'date', 'name', 'text', 'positive', 'rep', 'rtv', 'fav', 'total_count', 'fol', 'friends', 'lisy_count']\n",
        "df_positive = pd.read_csv('/content/drive/MyDrive/school21/positive.csv', sep = \";\", names = columns)\n",
        "df_negative = pd.read_csv('/content/drive/MyDrive/school21/negative.csv', sep = \";\", names = columns)\n",
        "df = pd.concat((df_positive, df_negative), axis=0)\n",
        "df.positive[df.positive == -1] = 0\n",
        "df.index = list(range(226834))"
      ],
      "metadata": {
        "id": "LUSB31nKMx-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.text = df.text.str.lower()\n",
        "df.text = df.text.str.replace(r\"[^А-Яа-я]\",\" \")\n",
        "df.text.loc[19:22]"
      ],
      "metadata": {
        "id": "jCZo8i8ONhbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сделаем токенизацию текста и удалим стоп-слова:\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "df.text = list(map(word_tokenize, df.text))\n",
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "russian_stopwords.sort()\n",
        "russian_stopwords\n",
        "\n",
        "def delete_stopword(words):\n",
        "    global russian_stopwords\n",
        "    new_s = [word for word in words if word not in russian_stopwords]\n",
        "    return new_s\n",
        "\n",
        "df.text = list(map(delete_stopword, df.text))\n",
        "df.text.loc[19:22]"
      ],
      "metadata": {
        "id": "kYRzHKg6NleI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Проведем лемматизацию полученных слов:\n",
        "\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def lemmatization(words):\n",
        "    global morph\n",
        "    new_s = [morph.parse(word)[0].normal_form for word in words]\n",
        "    return new_s\n",
        "\n",
        "df.text = list(map(lemmatization, df.text))\n",
        "df.text.loc[19:22]"
      ],
      "metadata": {
        "id": "a2PdF_2xNriP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_df = df[[\"text\", \"positive\"]]\n",
        "preprocessed_df"
      ],
      "metadata": {
        "id": "oFhzisgNNwDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "w2v_10_10 = Word2Vec(size=10, min_count=10)\n",
        "w2v_10_10.build_vocab(preprocessed_df.text)  # создадим словарь\n",
        "w2v_10_10.train(preprocessed_df.text, total_examples=w2v_10_10.corpus_count, epochs=100) # обучим модель на нашем наборе текстов\n",
        "w2v_10_10.save('w2v_10_10')\n",
        "\n",
        "# w2v_300_10 = Word2Vec(size=300, min_count=10) \n",
        "# w2v_300_10.build_vocab(preprocessed_df.text)\n",
        "# w2v_300_10.train(preprocessed_df.text, total_examples=w2v_300_10.corpus_count, epochs=100)\n",
        "# w2v_300_10.save('w2v_300_10')\n",
        "\n",
        "w2v_500_10 = Word2Vec(size=500, min_count=10) \n",
        "w2v_500_10.build_vocab(preprocessed_df.text)\n",
        "w2v_500_10.train(preprocessed_df.text, total_examples=w2v_500_10.corpus_count, epochs=100)\n",
        "w2v_500_10.save('w2v_500_10')\n",
        "\n",
        "w2v_300_1 = Word2Vec(size=300, min_count=1) \n",
        "w2v_300_1.build_vocab(preprocessed_df.text)\n",
        "w2v_300_1.train(preprocessed_df.text, total_examples=w2v_300_1.corpus_count, epochs=100)\n",
        "w2v_300_1.save('w2v_300_1')\n",
        "\n",
        "w2v_300_100 = Word2Vec(size=300, min_count=100) \n",
        "w2v_300_100.build_vocab(preprocessed_df.text)\n",
        "w2v_300_100.train(preprocessed_df.text, total_examples=w2v_300_100.corpus_count, epochs=100)\n",
        "w2v_300_100.save('w2v_300_100')\n",
        "\n",
        "w2v_10_10_n = Word2Vec.load('w2v_10_10')\n",
        "# w2v_300_10_n = Word2Vec.load('w2v_300_10')\n",
        "w2v_500_10_n = Word2Vec.load('w2v_500_10')\n",
        "w2v_300_1_n = Word2Vec.load('w2v_300_1')\n",
        "w2v_300_100_n = Word2Vec.load('w2v_300_100')\n"
      ],
      "metadata": {
        "id": "0wjegPhlN5YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_300_10 = Word2Vec(size=300, min_count=10) \n",
        "w2v_300_10.build_vocab(preprocessed_df.text)\n",
        "w2v_300_10.train(preprocessed_df.text, total_examples=w2v_300_10.corpus_count, epochs=100)\n",
        "w2v_300_10.save('w2v_300_10')\n",
        "\n",
        "w2v_300_10_n = Word2Vec.load('w2v_300_10')"
      ],
      "metadata": {
        "id": "6y_NVONdwM5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Выберем случайные слова из датафрейма\n",
        "import random\n",
        "\n",
        "set = []\n",
        "\n",
        "# Выберем пять позитивных слов\n",
        "for i in range(0, 5):\n",
        "  str_p = random.choice(list(preprocessed_df[df.positive == 1].text))\n",
        "  word = random.sample(str_p, 1)\n",
        "  set.append(word)\n",
        "\n",
        "# Выберем пять негативных слов\n",
        "for i in range(0, 5):\n",
        "  str_p = random.choice(list(preprocessed_df[df.positive == 0].text))\n",
        "  word = random.sample(str_p, 1)\n",
        "  set.append(word)\n",
        "\n",
        "set"
      ],
      "metadata": {
        "id": "sBMRQoA-N_zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Синонимы для модели w2v_300_1:\n",
        "\n",
        "for i in range(0, 10):\n",
        "  w2v_300_1.wv.most_similar(positive=set[i], topn=15)\n",
        "  #print(w2v_300_1.wv.most_similar(positive=set[i], topn=15))\n",
        "\n",
        "# Антонимы для модели w2v_300_1:\n",
        "\n",
        "for i in range(0, 10):\n",
        "  w2v_300_1.wv.most_similar(negative=set[i], topn=15)\n",
        "  #print(w2v_300_1.wv.most_similar(negative=set[i], topn=15))"
      ],
      "metadata": {
        "id": "UhdXC2P1OSkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Синонимы для модели w2v_300_10:\n",
        "\n",
        "for i in range(0, 10):\n",
        "  w2v_300_10.wv.most_similar(positive=set[i], topn=15)\n",
        "  #print(w2v_300_10.wv.most_similar(positive=set10[i], topn=15))\n",
        "\n",
        "# Антонимы для модели w2v_300_10:\n",
        "\n",
        "for i in range(0, 10):\n",
        "  w2v_300_10.wv.most_similar(negative=set[i], topn=15)\n",
        "  #print(w2v_300_10.wv.most_similar(negative=set10[i], topn=15))"
      ],
      "metadata": {
        "id": "emecG-_rOdz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set100 = [['сейчас'],\n",
        " ['папа'],\n",
        " ['забавный'],\n",
        " ['против'],\n",
        " ['нужный'],\n",
        " ['мой'],\n",
        " ['никто'],\n",
        " ['наушник'],\n",
        " ['есть'],\n",
        " ['быть']]"
      ],
      "metadata": {
        "id": "lLQQiHF7O-Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Синонимы для модели w2v_300_100:\n",
        "\n",
        "for i in range(0, 10):\n",
        "  w2v_300_100.wv.most_similar(positive=set100[i], topn=15)\n",
        "  #print(w2v_300_100.wv.most_similar(positive=set100[i], topn=15))\n",
        "\n",
        "# Антонимы для модели w2v_300_100:\n",
        "\n",
        "for i in range(0, 10):\n",
        "  w2v_300_100.wv.most_similar(negative=set100[i], topn=15)\n",
        "  #print(w2v_300_100.wv.most_similar(negative=set100[i], topn=15))"
      ],
      "metadata": {
        "id": "3Hw8-PcEOj2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Синонимы для модели w2v_10_10:\n",
        "\n",
        "for i in range(0, 10):\n",
        "  w2v_10_10.wv.most_similar(positive=set[i], topn=15)\n",
        "  #print(w2v_10_10.wv.most_similar(positive=set[i], topn=15))\n",
        "\n",
        "# Антонимы для модели w2v_10_10:\n",
        "\n",
        "for i in range(0, 10):\n",
        "  w2v_10_10.wv.most_similar(negative=set[i], topn=15)\n",
        "  #print(w2v_10_10.wv.most_similar(negative=set[i], topn=15))"
      ],
      "metadata": {
        "id": "eHeFKXOROrfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Синонимы для модели w2v_500_10:\n",
        "\n",
        "for i in range(0, 10):\n",
        "  w2v_500_10.wv.most_similar(positive=set[i], topn=15)\n",
        "  #print(w2v_500_10.wv.most_similar(positive=set10[i], topn=15))\n",
        "\n",
        "# Антонимы для модели w2v_500_10:\n",
        "\n",
        "for i in range(0, 10):\n",
        "  w2v_500_10.wv.most_similar(negative=set[i], topn=15)\n",
        "  #print(w2v_500_10.wv.most_similar(negative=set10[i], topn=15))"
      ],
      "metadata": {
        "id": "P-Jp1CinOxav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод.\n",
        "\n",
        "Чем больше размер результирующего пространства, тем выше точность определения синонимов/антонимов моделью потому, что у модели больше критериев оценки близости слов и можно это сделать точнее.\n",
        "\n",
        "Минимальная встречаемость слов влияет на точность определения синонимов/антонимов моделью так: чем больше минимальная встречаемость слов, тем больше точность определения синонимов/антонимов. Тем не менее, в результат могут не попасть слова, являющиеся наиболее точными синонимами/антонимамим, но не попавшие в выборку должное количество раз."
      ],
      "metadata": {
        "id": "axglf_f4-t3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import pylab\n",
        "pylab.rcParams['figure.figsize'] = (15, 10)\n",
        "\n",
        "\n",
        "def reduce_dimensions(w2v_model):\n",
        "    \"\"\"Фукнция принимает модель word2vec и возвращает массив абсцисс,\n",
        "    массив ординат и массив слов после снижения размерности\"\"\"\n",
        "    tsne = TSNE(n_components=2, random_state=256)  # создадим экземпляр модели TSNE\n",
        "    vectors = np.asarray(w2v_model.wv.vectors)     # возьмем из модели 300-мерный массив слов-векторов\n",
        "    labels = np.asarray(w2v_model.wv.index2word)   # отдельно сохраним соответствие номера вектора и самого слова\n",
        "    vectors = tsne.fit_transform(vectors)          # проведем преобразование каждого вектора в 2-мерный\n",
        "\n",
        "    x = [v[0] for v in vectors]                    # запишем отдельно массив абсцисс и массив ординат\n",
        "    y = [v[1] for v in vectors]\n",
        "    return x, y, labels\n",
        "\n",
        "\n",
        "def plot_w2v(w2v_model):\n",
        "    \"\"\"Функция строит график распределения слов по векторному пространству\n",
        "    размерности 2 исходя из обученной модели word2vec\"\"\"\n",
        "    x, y, labels = reduce_dimensions(w2v_model)                      # получим значения по осям и названия точек (исходные слова)\n",
        "    plt.scatter(x, y)                                                # строим график с точками\n",
        "    words_to_show_indices = np.random.randint(len(labels), size=25)  # выберем 25 случайных слов, которые отобразим на графике\n",
        "    for i in words_to_show_indices:\n",
        "        plt.annotate(labels[i], (x[i], y[i]))                        # для каждого из этих 25 слов отобразим текст на картинке\n",
        "    plt.show()\n",
        "\n",
        "plot_w2v(w2v_10_10)"
      ],
      "metadata": {
        "id": "LW3RiOYKO5Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_w2v(w2v_300_10)"
      ],
      "metadata": {
        "id": "UeZFcfdgdH1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_w2v(w2v_500_10)"
      ],
      "metadata": {
        "id": "C-YIYMEDdMeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_w2v(w2v_300_1)"
      ],
      "metadata": {
        "id": "9RvjFCM9dSC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_w2v(w2v_300_100)"
      ],
      "metadata": {
        "id": "ac27ggp1dUiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод.\n",
        "\n",
        "Чем больше минимальная встречаемость слов, тем реже расположены точки на графике. Это происходит в первую очередь потому, что в рассчет берется меньшее количество слов.\n",
        "\n",
        "Чем больше размер результирующего пространства, тем более явно наблюдается уплотнение точек ближе к центру. Это связано с тем, что с увеличением размера результирующего пространства повышается точность определения связей между словами. Большой размер результирующего пространства может вызывать переобученность."
      ],
      "metadata": {
        "id": "e1W9si9o-hd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "w2v = Word2Vec(size=300, min_count=2) \n",
        "w2v.build_vocab(preprocessed_df.text)"
      ],
      "metadata": {
        "id": "saPcn_5ldWow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим модель по умолчанию и попробуем предсказать продолжение случайной фразы:"
      ],
      "metadata": {
        "id": "FxM-ujsA-8LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.train(preprocessed_df.text, total_examples=preprocessed_df.shape[0], epochs=100)\n",
        "w2v.predict_output_word([\"такси\", \"везти\", \"работа\"])"
      ],
      "metadata": {
        "id": "p-coSMY_4xdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.train(preprocessed_df.text, total_examples=preprocessed_df.shape[0], epochs=10)\n",
        "w2v.predict_output_word([\"такси\", \"везти\", \"работа\"])"
      ],
      "metadata": {
        "id": "Zb-nUWeJ4zMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем предсказать продолжение твита \"Котёнка вчера носик разбила, плакала и расстраивалась :(\""
      ],
      "metadata": {
        "id": "IGm-ldcx_WmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.train(preprocessed_df.text, total_examples=preprocessed_df.shape[0], epochs=10)\n",
        "w2v.predict_output_word([\"вчера\", \"носик\", \"разбить\"])"
      ],
      "metadata": {
        "id": "g0STtL8v9vuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.train(preprocessed_df.text, total_examples=preprocessed_df.shape[0], epochs=100)\n",
        "w2v.predict_output_word([\"вчера\", \"носик\", \"разбить\"])"
      ],
      "metadata": {
        "id": "md_p9SOp928h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чем больше эпох обучения, тем более точное предсказание дает модель."
      ],
      "metadata": {
        "id": "niH-NbO2_8FR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Решение задания №2\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install xgboost\n",
        "\n",
        "tweets = [TaggedDocument(doc, [i]) for i, doc in enumerate(preprocessed_df.text)]\n",
        "\n",
        "# Разобьем набор текстов на тренировочную и тестовую выборки\n",
        "X_train_texts, X_test_texts, y_train, y_test = train_test_split(preprocessed_df.text, preprocessed_df.positive, test_size=0.2, random_state=21)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooG-axvj-W4q",
        "outputId": "138455df-2eec-4a68-a96a-b4636e1866f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Создадим модель Doc2Vec с минимальной встречаемостью слова 2\n",
        "d2v = Doc2Vec(tweets, min_count=2)\n",
        "d2v.train(tweets, total_examples=len(tweets), epochs=20)  # подберем веса коэффициентов внутри модели, которые больше будут подходить к нашему набору текстов"
      ],
      "metadata": {
        "id": "f9YMb3nCAdGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_text_array_to_vector_dataframe(text_array):\n",
        "    \"\"\"Функция, которая преобразует одномерную колонку списков слов из текстов\n",
        "    в датафрейм со значениями векторов этих текстов\"\"\"\n",
        "    columns = [str(n) for n in range(d2v.vector_size)]               # задаем список названий колонок - просто порядковые номера\n",
        "    vectors_ndarray = text_array.apply(d2v.infer_vector).to_list()  # прогоняем каждый текст через модель doc2vec и формируем многомерный массив чисел\n",
        "    return pd.DataFrame(vectors_ndarray, columns=columns)            # оборачиваем его в датафрейм для удобства\n",
        "\n",
        "\n",
        "X_train = transform_text_array_to_vector_dataframe(X_train_texts)    # наконец создадим датафреймы, которые сможем подать в модель классификации\n",
        "X_test = transform_text_array_to_vector_dataframe(X_test_texts)"
      ],
      "metadata": {
        "id": "cjHV-0bSAs_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Применим модель xgboost. Основные гиперпараметры: max_depth - максимальная глубина деревьев модели и n_estimators - количество деревьев.\n",
        "# min_count=2, epochs=20\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "xgb = XGBClassifier(max_depth=10, n_estimators=50)\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred = xgb.predict(X_test)\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "metadata": {
        "id": "rEh3Yzi5AwxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем поменять параметр минимальная встречаемость слова в текстах в doc2vec"
      ],
      "metadata": {
        "id": "1TZG5-wUA5kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Попробуем минимальную встречаемость 10\n",
        "# min_count=10, epochs=20\n",
        "\n",
        "d2v = Doc2Vec(tweets, min_count=10)\n",
        "d2v.train(tweets, total_examples=len(tweets), epochs=20)\n",
        "\n",
        "X_train10 = transform_text_array_to_vector_dataframe(X_train_texts)\n",
        "X_test10 = transform_text_array_to_vector_dataframe(X_test_texts)\n",
        "\n",
        "xgb.fit(X_train10, y_train)\n",
        "y_pred = xgb.predict(X_test10)\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "metadata": {
        "id": "G-PmA3ymA1kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Попробуем минимальную встречаемость 100\n",
        "# min_count=100, epochs=20\n",
        "\n",
        "d2v = Doc2Vec(tweets, min_count=100)\n",
        "d2v.train(tweets, total_examples=len(tweets), epochs=20)\n",
        "\n",
        "X_train100 = transform_text_array_to_vector_dataframe(X_train_texts)\n",
        "X_test100 = transform_text_array_to_vector_dataframe(X_test_texts)\n",
        "\n",
        "xgb.fit(X_train100, y_train)\n",
        "y_pred = xgb.predict(X_test100)\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "metadata": {
        "id": "MY9-ge2tBAQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем разные модели, меняя гиперпараметры максимальная глубина деревеьев в бустинге и количество деревьев в бустинге"
      ],
      "metadata": {
        "id": "CCxiGKLmBGri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Применим параметры, общие для всех min_count=10, epochs=10\n",
        "\n",
        "d2v = Doc2Vec(tweets, min_count=10)\n",
        "d2v.train(tweets, total_examples=len(tweets), epochs=10)\n",
        "\n",
        "X_train = transform_text_array_to_vector_dataframe(X_train_texts)\n",
        "X_test = transform_text_array_to_vector_dataframe(X_test_texts)"
      ],
      "metadata": {
        "id": "PZPo0H7RBDT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier(max_depth=5, n_estimators=50)\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred = xgb.predict(X_test)\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "metadata": {
        "id": "hmY8i7bLBKoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier(max_depth=10, n_estimators=50)\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred = xgb.predict(X_test)\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "metadata": {
        "id": "Z8oGp8Y7BNPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier(max_depth=10, n_estimators=10)\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred = xgb.predict(X_test)\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "metadata": {
        "id": "QDudDvKdBPSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier(max_depth=10, n_estimators=100)\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred = xgb.predict(X_test)\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "metadata": {
        "id": "9daw8C15BRnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод.\n",
        "\n",
        "Лучше использовать более глубокие деревья в модели градиентного бустинга. Увеличение количества деревьев также улучшает результат"
      ],
      "metadata": {
        "id": "bSv00DcGBYLg"
      }
    }
  ]
}