{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "d06_task-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Анализ текстов с использованием метода Word2vec\n",
        "\n",
        "Word2vec - это еще один агоритм преобразования текстов в точки в векторном пространстве признаков. В его основе лежит нейронная сеть - автоэнкодер, который преобразует слово в вектор фиксированной длины. Особенность этого алгоритма заключается в том, что он учитывает семантику слов. Близкие по смыслу слова будут располагаться ближе друг к другу в векторном пространстве, а далекие - дальше. Если объем текстов достаточно большой, то с помощью модели word2vec мы можем определять синонимы и антонимы слов, и, благодаря этому, точность классификации текстов может увеличиться.\n",
        "\n",
        "Сегодня будем пользоваться теми же наборами текстов, что и в прошлый раз. Прежде всего установим уже известные вам библиотеки для работы с текстовыми данными.\n"
      ],
      "metadata": {
        "id": "KFgj2IhZoEJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2 nltk\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "id": "izBRyoiHyfxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сейчас мы готовы к проведению предобработки текста. Но для того, чтобы работать с моделью word2vec, нам еще понадобится установить библиотеку gensim, в которой описана эта модель."
      ],
      "metadata": {
        "id": "NEDnZjcJzHgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "4FDadZjZLY6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для демонстрации возможностей этой библиотеки снова возьмем набор данных с текстами твитов, где сразу же переименуем класс -1 в 0. Вспомним, как выглядит наш датасет."
      ],
      "metadata": {
        "id": "uKxOHCADM6gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "# drive.mount('/content/drive/')\n",
        "drive.mount(\"/content/drive/\", force_remount=True)"
      ],
      "metadata": {
        "id": "hAIFQoNTgd-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('/content/drive/MyDrive/data/tweets_example.xlsx')\n",
        "df.positive[df.positive==-1] = 0\n",
        "df.loc[16:25]"
      ],
      "metadata": {
        "id": "MrOFcawaLo1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Так же, как и при использовании изученных раньше алгоритмов векторизации текста, нам нужно провести очистку данных, т.е. привести все слова текстов к одинаковому виду и форме. Напишем для этого функцию, чтобы можно было потом все эти действия вызывать одной командой."
      ],
      "metadata": {
        "id": "IUBBZHDajqbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pymorphy2\n",
        "\n",
        "\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "\n",
        "def text_preprocessing(text):\n",
        "    \"\"\"Функция принимает строку и возвращает список слов в начальной форме\"\"\"\n",
        "    text = text.lower()                                                         # приводим текст к нижнему регистру\n",
        "    text = re.sub(r\"[^А-Яа-я]\", \" \", text)                                      # удаляем все некириллические символы\n",
        "    words = word_tokenize(text)                                                 # разбиваем тексты на списки слов\n",
        "    words = [morph.parse(word)[0].normal_form for word in words]                # приводим слова к начальной форме\n",
        "    words = [word for word in words if word not in stopwords.words(\"russian\")]  # удаляем слова из стоп-листа\n",
        "    return words\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "vOThpNTu51g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_df = df[[\"text\", \"positive\"]]\n",
        "preprocessed_df.text = df.text.apply(text_preprocessing)\n",
        "preprocessed_df[15:25]"
      ],
      "metadata": {
        "id": "Km1LPs1XiWuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обратите внимание, на этот раз мы не удаляем редкие слова, поскольку теперь текст характеризуется не частотностью тех или иных слов, а семантикой слов, входящих в данный текст.\n",
        "\n",
        "Теперь обучим модель word2vec. Поскольку это не предиктивная модель, будем обучать ее на всём множестве текстов, чтобы получить векторное представление как можно большего количества слов. Подробно о параметрах модели можно почитать в официальной [документации](https://radimrehurek.com/gensim/models/word2vec.html)."
      ],
      "metadata": {
        "id": "pHEm66mM1G5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "w2v = Word2Vec(size=300, min_count=1)  # создадим экземпляр модели word2vec. Здесь size - размер векторного пространства,\n",
        "                                       # min_count - минимальное количество появлений слова в наборе данных, при котором\n",
        "                                       # будем учитывать это слово в модели\n",
        "w2v.build_vocab(preprocessed_df.text)  # обучим модель на нашем наборе текстов"
      ],
      "metadata": {
        "id": "jFA3rDXS1Fkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь посмотрим, какую информацию о словах мы можем получить из обученной модели. Например, для выбранного нами слова можем посмотреть список наиболее похожих на него слов с точки зрения модели."
      ],
      "metadata": {
        "id": "kGs2ZbBZqZ7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.wv.most_similar(positive=\"ерунда\")  # этот метод возвращает список кортежей, где первый элемент - это слово,\n",
        "                                        # а второй - степень схожести со словом \"ерунда\". Чем ближе это число к 1,\n",
        "                                        # тем ближе по смыслу выведенное слово"
      ],
      "metadata": {
        "id": "O-Wkf1KjlHQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Аналогично можем посмотреть наименее похожие слова:"
      ],
      "metadata": {
        "id": "7zQiScyE2BGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.wv.most_similar(negative=[\"ерунда\"])  # аргумент negative говорит о том, что нужно искать наименее похожие слова.\n",
        "                                          # В этом случае числа - это степень непохожести.\n",
        "                                          # Чем ближе к 1, тем слово меньше похоже на \"ерунду\""
      ],
      "metadata": {
        "id": "8NjTqIMQ1N7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метод `most_similar` может помочь нам определить слова, которые наиболее похожи на один набор слов и наименее похожи на другой набор слов."
      ],
      "metadata": {
        "id": "vFYtDT4m6FcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.wv.most_similar(positive=[\"подняться\", \"угата\"], negative=[\"погибать\", \"клоун\"])\n",
        "\n",
        "# на этот раз цифры - это некая общая метрика похожести на то, что мы просим"
      ],
      "metadata": {
        "id": "BbrE5uX-6W1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Еще можно посмотреть, насколько похожи два слова из выборки"
      ],
      "metadata": {
        "id": "5DlsjpnJ5-ML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.wv.similarity(\"клоун\", \"работа\")  # ответ может быть отрицательным - это будет означать, что\n",
        "                                         # эта пара слов - больше антонимы, чем синонимы"
      ],
      "metadata": {
        "id": "uvhQtHCj6xhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Очевидно, что с теми словами, которых не было в обучающей выборке, модель работать не сможет:"
      ],
      "metadata": {
        "id": "5bMxQ15mx5j0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.wv.most_similar(\"сбербанк\")"
      ],
      "metadata": {
        "id": "Ivv3qJpLx4al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "А теперь давайте посмотрим на график, какие слова как расположены друг относительно друга. По умолчанию модель word2vec отображает все слова в пространство размерности 300. Это означает, что каждое слово превращается в набор из 300 чисел. На мониторе такую размерность отобразить очень сложно, поэтому воспользуемся методом снижения размерности векторного пространства t-SNE. Сожмем наши вектора до размерности 2, чтобы их легко можно было отобразить на плоскости."
      ],
      "metadata": {
        "id": "bmhcJrBO-m-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import pylab\n",
        "pylab.rcParams['figure.figsize'] = (15, 10)\n",
        "\n",
        "\n",
        "def reduce_dimensions(w2v_model):\n",
        "    \"\"\"Фукнция принимает модель word2vec и возвращает массив абсцисс,\n",
        "    массив ординат и массив слов после снижения размерности\"\"\"\n",
        "    tsne = TSNE(n_components=2, random_state=256)  # создадим экземпляр модели TSNE\n",
        "    vectors = np.asarray(w2v_model.wv.vectors)     # возьмем из модели 300-мерный массив слов-векторов\n",
        "    labels = np.asarray(w2v_model.wv.index2word)   # отдельно сохраним соответствие номера вектора и самого слова\n",
        "    vectors = tsne.fit_transform(vectors)          # проведем преобразование каждого вектора в 2-мерный\n",
        "\n",
        "    x = [v[0] for v in vectors]                    # запишем отдельно массив абсцисс и массив ординат\n",
        "    y = [v[1] for v in vectors]\n",
        "    return x, y, labels\n",
        "\n",
        "\n",
        "def plot_w2v(w2v_model):\n",
        "    \"\"\"Функция строит график распределения слов по векторному пространству\n",
        "    размерности 2 исходя из обученной модели word2vec\"\"\"\n",
        "    x, y, labels = reduce_dimensions(w2v_model)                      # получим значения по осям и названия точек (исходные слова)\n",
        "    plt.scatter(x, y)                                                # строим график с точками\n",
        "    words_to_show_indices = np.random.randint(len(labels), size=25)  # выберем 25 случайных слов, которые отобразим на графике\n",
        "    for i in words_to_show_indices:\n",
        "        plt.annotate(labels[i], (x[i], y[i]))                        # для каждого из этих 25 слов отобразим текст на картинке\n",
        "\n",
        "\n",
        "plot_w2v(w2v)                                                        # применим написанные функции к обученной модели"
      ],
      "metadata": {
        "id": "zGEvziOxEVoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Кроме вышеперечисленных возможностей, можно обучить модель word2vec на предсказание следующих слов. Обучим ее и попробуем предсказать продолжение твита \"Котёнка вчера носик разбила, плакала и расстраивалась :(\""
      ],
      "metadata": {
        "id": "RVS2lILcu1CA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.train(preprocessed_df.text, total_examples=preprocessed_df.shape[0], epochs=10000)\n",
        "w2v.predict_output_word([\"вчера\", \"носик\", \"разбить\"])"
      ],
      "metadata": {
        "id": "mzpOg4SRvKEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вообще говоря, списки синонимов и антонимов у нас получились достаточно спорные. Причина этого - малая выборка слов, на которых обучалась модель. Внутри word2vec используется нейросеть-автоэнкодер, а таким моделям всегда нужно много данных для того, чтобы составить корректное отображение входных данных в векторное пространство. Чем больше слов в тексте, тем понятней, какие слова похожи по значению, а какие наоборот противоположны."
      ],
      "metadata": {
        "id": "eL2mANxvTMve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 1\n",
        "1. Проведите предобработку текстов из файлов positive.csv, negative.csv. Нужно выполнить те же действия, что в предыдущем дне, но не удалять редко встречающиеся слова. Регулировать использование редких слов будем на уровне модели word2vec. Не забудьте удалить стоп-слова.\n",
        "2. Будем исследовать то, как влияют на качество преобразования *размер целевого векторного пространства* и *использование редких слов*. Создайте несколько моделей word2vec, перебрав параметры:\n",
        "  - размер результирующего пространства: [10, 300, 500] при фиксированной минимальной встречаемости слов = 10\n",
        "  - минимальная встречаемость слов: [1, 10, 100] при фиксированном размере результирующего векторного пространства = 300\n",
        "\n",
        "  Обучите их на всем пространстве текстов.\n",
        "\n",
        "3. Отберите 5 случайных слов из выборки позитивных публикаций и 5 случайных слови из выборки негативных публикаций.\n",
        "4. Для каждой из обученных моделей найдите по 15 синонимов и по 15 антонимов для каждого из слов из п.3. Опишите:\n",
        "  - как влияет размер результирующего пространства на точность определения синонимов/антонимов моделью? почему?\n",
        "  - как влияет минимальная встречаемость слов на точность определения синонимов/антонимов моделью? почему?\n",
        "5. Постройте графики распределения слов в двумерном пространстве. Опишите, как влияют исследуемые параметры на кучность и расположение точек на графике. Почему?\n",
        "6. Возьмите любой твит, обучите модель word2vec с параметрами по умолчанию и попробуйте предсказать продолжение твита. Также попробуйте предсказать продолжение случайной фразы. Сравните результаты, полученные после обучения моделей с разным количеством эпох обучения."
      ],
      "metadata": {
        "id": "basdbO5LXQHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2 nltk\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "!pip install gensim\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount(\"/content/drive/\", force_remount=True)"
      ],
      "metadata": {
        "id": "RbTyuv6Nrn2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "columns = ['id', 'date', 'name', 'text', 'positive', 'rep', 'rtv', 'fav', 'total_count', 'fol', 'friends', 'lisy_count']\n",
        "df_positive = pd.read_csv('/content/drive/MyDrive/school21/positive.csv', sep = \";\", names = columns)\n",
        "df_negative = pd.read_csv('/content/drive/MyDrive/school21/negative.csv', sep = \";\", names = columns)\n",
        "df = pd.concat((df_positive, df_negative), axis=0)\n",
        "df.positive[df.positive == -1] = 0\n",
        "df.index = list(range(226834))"
      ],
      "metadata": {
        "id": "HTdBDPwEzixD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.text = df.text.str.lower()\n",
        "df.text = df.text.str.replace(r\"[^А-Яа-я]\",\" \")\n",
        "df.text.loc[19:22]"
      ],
      "metadata": {
        "id": "l2KDdye-sSku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel (r'\\Users\\ebalgruu\\Desktop\\clean_tweets_df.xlsx', index = False, header=True)\n",
        "df.to_csv('clean_dataframe.csv', sep='\\t', encoding='utf-8')\n",
        "df.to_pickle('/content/drive/MyDrive/data/clean_tweets_df.pkl')\n",
        "\n",
        "# df = pd.read_pickle('/content/drive/MyDrive/data/clean_tweets_df.pkl')"
      ],
      "metadata": {
        "id": "_K6nlwRM2A1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cJKu33zymL7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "размер результирующего пространства: [10, 300, 500] при фиксированной минимальной встречаемости слов = 10\n",
        "\n",
        "минимальная встречаемость слов: [1, 10, 100] при фиксированном размере результирующего векторного пространства = 300\n",
        "\n"
      ],
      "metadata": {
        "id": "zxfwKXbbmEYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Токенизация текста и удаление стоп-слов:\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "df.text = list(map(word_tokenize, df.text))\n",
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "russian_stopwords.sort()\n",
        "russian_stopwords\n",
        "\n",
        "def delete_stopword(words):\n",
        "    global russian_stopwords\n",
        "    new_s = [word for word in words if word not in russian_stopwords]\n",
        "    return new_s\n",
        "\n",
        "df.text = list(map(delete_stopword, df.text))\n",
        "df.text.loc[19:22]"
      ],
      "metadata": {
        "id": "_5jHG9jm0g-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Лемматизация:\n",
        "\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def lemmatization(words):\n",
        "    global morph\n",
        "    new_s = [morph.parse(word)[0].normal_form for word in words]\n",
        "    return new_s\n",
        "\n",
        "df.text = list(map(lemmatization, df.text))\n",
        "df.text.loc[19:22]"
      ],
      "metadata": {
        "id": "KQqrltwefwth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_df = df[[\"text\", \"positive\"]]\n",
        "preprocessed_df"
      ],
      "metadata": {
        "id": "LGOcJmAMf2J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "w2v_10_10 = Word2Vec(size=10, min_count=10)\n",
        "w2v_10_10.build_vocab(preprocessed_df.text)  # создадим словарь\n",
        "w2v_10_10.train(preprocessed_df.text, total_examples=w2v_10_10.corpus_count, epochs=100) # обучим модель на нашем наборе текстов\n",
        "w2v_10_10.save('w2v_10_10')\n",
        "\n",
        "# w2v_300_10 = Word2Vec(size=300, min_count=10) \n",
        "# w2v_300_10.build_vocab(preprocessed_df.text)\n",
        "# w2v_300_10.train(preprocessed_df.text, total_examples=w2v_300_10.corpus_count, epochs=100)\n",
        "# w2v_300_10.save('w2v_300_10')\n",
        "\n",
        "w2v_500_10 = Word2Vec(size=500, min_count=10) \n",
        "w2v_500_10.build_vocab(preprocessed_df.text)\n",
        "w2v_500_10.train(preprocessed_df.text, total_examples=w2v_500_10.corpus_count, epochs=100)\n",
        "w2v_500_10.save('w2v_500_10')\n",
        "\n",
        "w2v_300_1 = Word2Vec(size=300, min_count=1) \n",
        "w2v_300_1.build_vocab(preprocessed_df.text)\n",
        "w2v_300_1.train(preprocessed_df.text, total_examples=w2v_300_1.corpus_count, epochs=100)\n",
        "w2v_300_1.save('w2v_300_1')\n",
        "\n",
        "w2v_300_100 = Word2Vec(size=300, min_count=100) \n",
        "w2v_300_100.build_vocab(preprocessed_df.text)\n",
        "w2v_300_100.train(preprocessed_df.text, total_examples=w2v_300_100.corpus_count, epochs=100)\n",
        "w2v_300_100.save('w2v_300_100')\n",
        "\n",
        "w2v_10_10_n = Word2Vec.load('w2v_10_10')\n",
        "# w2v_300_10_n = Word2Vec.load('w2v_300_10')\n",
        "w2v_500_10_n = Word2Vec.load('w2v_500_10')\n",
        "w2v_300_1_n = Word2Vec.load('w2v_300_1')\n",
        "w2v_300_100_n = Word2Vec.load('w2v_300_100')"
      ],
      "metadata": {
        "id": "25faUcdN1Iie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_300_10 = Word2Vec(size=300, min_count=10) \n",
        "w2v_300_10.build_vocab(preprocessed_df.text)\n",
        "w2v_300_10.train(preprocessed_df.text, total_examples=w2v_300_10.corpus_count, epochs=100)\n",
        "w2v_300_10.save('w2v_300_10')\n",
        "\n",
        "w2v_300_10_n = Word2Vec.load('w2v_300_10')"
      ],
      "metadata": {
        "id": "DkgMMEFg1Jd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Случайные слова из датафрейма\n",
        "import random\n",
        "\n",
        "set = []\n",
        "\n",
        "# 5 позитивных слов\n",
        "for i in range(0, 5):\n",
        "  str_p = random.choice(list(preprocessed_df[df.positive == 1].text))\n",
        "  word = random.sample(str_p, 1)\n",
        "  set.append(word)\n",
        "\n",
        "# 5 негативных слов\n",
        "for i in range(0, 5):\n",
        "  str_p = random.choice(list(preprocessed_df[df.positive == 0].text))\n",
        "  word = random.sample(str_p, 1)\n",
        "  set.append(word)\n",
        "\n",
        "set"
      ],
      "metadata": {
        "id": "Qx_WOTsTf_n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Синонимы для модели w2v_300_1:\n",
        "\n",
        "for i in range(0, 10):\n",
        "  w2v_300_1.wv.most_similar(positive=set[i], topn=15)\n",
        "  #print(w2v_300_1.wv.most_similar(positive=set[i], topn=15))\n",
        "\n",
        "# Антонимы для модели w2v_300_1:\n",
        "\n",
        "for i in range(0, 10):\n",
        "  w2v_300_1.wv.most_similar(negative=set[i], topn=15)\n",
        "  #print(w2v_300_1.wv.most_similar(negative=set[i], topn=15))"
      ],
      "metadata": {
        "id": "YPYKdHUbgGj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Синонимы для модели w2v_300_10:\n",
        "\n",
        "for i in range(0, 10):\n",
        "  w2v_300_10.wv.most_similar(positive=set[i], topn=15)\n",
        "  #print(w2v_300_10.wv.most_similar(positive=set10[i], topn=15))\n",
        "\n",
        "# Антонимы для модели w2v_300_10:\n",
        "\n",
        "for i in range(0, 10):\n",
        "  w2v_300_10.wv.most_similar(negative=set[i], topn=15)\n",
        "  #print(w2v_300_10.wv.most_similar(negative=set10[i], topn=15))"
      ],
      "metadata": {
        "id": "SdZ_40rygJHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set100 = [['сейчас'],\n",
        " ['папа'],\n",
        " ['забавный'],\n",
        " ['против'],\n",
        " ['нужный'],\n",
        " ['мой'],\n",
        " ['никто'],\n",
        " ['наушник'],\n",
        " ['есть'],\n",
        " ['быть']]"
      ],
      "metadata": {
        "id": "ccvpZxPagLCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Синонимы для модели w2v_300_100:\n",
        "\n",
        "for i in range(0, 10):\n",
        "  w2v_300_100.wv.most_similar(positive=set100[i], topn=15)\n",
        "  #print(w2v_300_100.wv.most_similar(positive=set100[i], topn=15))\n",
        "\n",
        "# Антонимы для модели w2v_300_100:\n",
        "\n",
        "for i in range(0, 10):\n",
        "  w2v_300_100.wv.most_similar(negative=set100[i], topn=15)\n",
        "  #print(w2v_300_100.wv.most_similar(negative=set100[i], topn=15))"
      ],
      "metadata": {
        "id": "TCR6il04gNBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Синонимы для модели w2v_10_10:\n",
        "\n",
        "for i in range(0, 10):\n",
        "  w2v_10_10.wv.most_similar(positive=set[i], topn=15)\n",
        "  #print(w2v_10_10.wv.most_similar(positive=set[i], topn=15))\n",
        "\n",
        "# Антонимы для модели w2v_10_10:\n",
        "\n",
        "for i in range(0, 10):\n",
        "  w2v_10_10.wv.most_similar(negative=set[i], topn=15)\n",
        "  #print(w2v_10_10.wv.most_similar(negative=set[i], topn=15))"
      ],
      "metadata": {
        "id": "QLC1vBKagPFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Синонимы для модели w2v_500_10:\n",
        "\n",
        "for i in range(0, 10):\n",
        "  w2v_500_10.wv.most_similar(positive=set[i], topn=15)\n",
        "  #print(w2v_500_10.wv.most_similar(positive=set10[i], topn=15))\n",
        "\n",
        "# Антонимы для модели w2v_500_10:\n",
        "\n",
        "for i in range(0, 10):\n",
        "  w2v_500_10.wv.most_similar(negative=set[i], topn=15)\n",
        "  #print(w2v_500_10.wv.most_similar(negative=set10[i], topn=15))"
      ],
      "metadata": {
        "id": "PrZu8JrkgQxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод.\n",
        "Чем больше размер результирующего пространства, тем выше точность определения синонимов/антонимов моделью потому, что у модели больше критериев оценки близости слов и можно это сделать точнее.\n",
        "Минимальная встречаемость слов влияет на точность определения синонимов/антонимов моделью так: чем больше минимальная встречаемость слов, тем больше точность определения синонимов/антонимов. Тем не менее, в результат могут не попасть слова, являющиеся наиболее точными синонимами/антонимамим, но не попавшие в выборку должное количество раз."
      ],
      "metadata": {
        "id": "I9xX82G3gTG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import pylab\n",
        "pylab.rcParams['figure.figsize'] = (15, 10)\n",
        "\n",
        "\n",
        "def reduce_dimensions(w2v_model):\n",
        "    \"\"\"Фукнция принимает модель word2vec и возвращает массив абсцисс,\n",
        "    массив ординат и массив слов после снижения размерности\"\"\"\n",
        "    tsne = TSNE(n_components=2, random_state=256)  # создадим экземпляр модели TSNE\n",
        "    vectors = np.asarray(w2v_model.wv.vectors)     # возьмем из модели 300-мерный массив слов-векторов\n",
        "    labels = np.asarray(w2v_model.wv.index2word)   # отдельно сохраним соответствие номера вектора и самого слова\n",
        "    vectors = tsne.fit_transform(vectors)          # проведем преобразование каждого вектора в 2-мерный\n",
        "\n",
        "    x = [v[0] for v in vectors]                    # запишем отдельно массив абсцисс и массив ординат\n",
        "    y = [v[1] for v in vectors]\n",
        "    return x, y, labels\n",
        "\n",
        "\n",
        "def plot_w2v(w2v_model):\n",
        "    \"\"\"Функция строит график распределения слов по векторному пространству\n",
        "    размерности 2 исходя из обученной модели word2vec\"\"\"\n",
        "    x, y, labels = reduce_dimensions(w2v_model)                      # получим значения по осям и названия точек (исходные слова)\n",
        "    plt.scatter(x, y)                                                # строим график с точками\n",
        "    words_to_show_indices = np.random.randint(len(labels), size=25)  # выберем 25 случайных слов, которые отобразим на графике\n",
        "    for i in words_to_show_indices:\n",
        "        plt.annotate(labels[i], (x[i], y[i]))                        # для каждого из этих 25 слов отобразим текст на картинке\n",
        "    plt.show()\n",
        "\n",
        "plot_w2v(w2v_10_10)"
      ],
      "metadata": {
        "id": "VFSsFXoRgXUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_w2v(w2v_300_10)"
      ],
      "metadata": {
        "id": "F-WP7OTqgZjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_w2v(w2v_500_10)"
      ],
      "metadata": {
        "id": "A4RPXJlbgbUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_w2v(w2v_300_1)"
      ],
      "metadata": {
        "id": "nT-1yfRxgc24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_w2v(w2v_300_100)"
      ],
      "metadata": {
        "id": "8cRddpWAgek_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод.\n",
        "Чем больше минимальная встречаемость слов, тем реже расположены точки на графике. Это происходит в первую очередь потому, что в рассчет берется меньшее количество слов.\n",
        "Чем больше размер результирующего пространства, тем более явно наблюдается уплотнение точек ближе к центру. Это связано с тем, что с увеличением размера результирующего пространства повышается точность определения связей между словами. Большой размер результирующего пространства может вызывать переобученность."
      ],
      "metadata": {
        "id": "nesfJqd-gg0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим модель по умолчанию и попробуем предсказать продолжение случайной фразы:\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_JLa2pKVhKn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "w2v = Word2Vec(size=300, min_count=2) \n",
        "w2v.build_vocab(preprocessed_df.text)"
      ],
      "metadata": {
        "id": "cVp3v9a0gjnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.train(preprocessed_df.text, total_examples=preprocessed_df.shape[0], epochs=100)\n",
        "w2v.predict_output_word([\"такси\", \"везти\", \"работа\"])"
      ],
      "metadata": {
        "id": "51u2yQT4g2rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.train(preprocessed_df.text, total_examples=preprocessed_df.shape[0], epochs=10)\n",
        "w2v.predict_output_word([\"такси\", \"везти\", \"работа\"])"
      ],
      "metadata": {
        "id": "cIaUKI1Rg4kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем предсказать продолжение твита \"Котёнка вчера носик разбила, плакала и расстраивалась :(\"\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "b548jnETg66y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.train(preprocessed_df.text, total_examples=preprocessed_df.shape[0], epochs=10)\n",
        "w2v.predict_output_word([\"вчера\", \"носик\", \"разбить\"])"
      ],
      "metadata": {
        "id": "rtUPAmdfg8jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.train(preprocessed_df.text, total_examples=preprocessed_df.shape[0], epochs=100)\n",
        "w2v.predict_output_word([\"вчера\", \"носик\", \"разбить\"])"
      ],
      "metadata": {
        "id": "r7u7BM0kg_UQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чем больше эпох обучения, тем более точное предсказание дает модель.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "uq1KsnrOhF4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Использование градиентного бустинга над решающими деревьями для решения задачи классификации текстов\n",
        "\n",
        "В предыдущем разделе мы научились превращать слова в векторы. Но перед нами стоит задача классификации текста, а не одного слова, поэтому нам нужно придумать способ, как целый текст представить числами.\n",
        "\n",
        "После преобразования отдельные слова стали векторами, значения которых зависят от семантики слова. Будем рассматривать твит как сущность с усредненной семантикой всех содержащихся в нём слов. Таким образом, для преобразования целого текста в вектор, нам нужно получить средний вектор всех содержащихся в нём слов. Такой способ реализован в модели Doc2Vec в библиотеке gensim."
      ],
      "metadata": {
        "id": "OpZspJounApt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "tweets = [TaggedDocument(doc, [i]) for i, doc in enumerate(preprocessed_df.text)]  # преобразуем наши тексты в объекты, понятные док-2-веку\n",
        "d2v = Doc2Vec(tweets, min_count=2)                        # создадим модель Doc2Vec\n",
        "d2v.train(tweets, total_examples=len(tweets), epochs=20)  # подберем веса коэффициентов внутри модели, которые больше будут подходить к нашему набору текстов"
      ],
      "metadata": {
        "id": "WayTnYoIE_Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# разобьем набор текстов на тренировочную и тестовую выборки\n",
        "X_train_texts, X_test_texts, y_train, y_test = train_test_split(preprocessed_df.text, preprocessed_df.positive, test_size=0.2, random_state=21)"
      ],
      "metadata": {
        "id": "IJq2h1nKtCkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_texts  # пока что наши тексты выглядят как списки слов в начальной форме, но нам нужно получить из этого векторы"
      ],
      "metadata": {
        "id": "9ChvJJgHDlRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_text_array_to_vector_dataframe(text_array):\n",
        "    \"\"\"Функция, которая преобразует одномерный колонку списков слов из текстов\n",
        "    в датафрейм со значениями векторов этих текстов\"\"\"\n",
        "    columns = [str(n) for n in range(d2v.vector_size)]               # задаем список названий колонок - просто порядковые номера\n",
        "    vectors_ndarray = text_array.apply(d2v.infer_vector).to_list()  # прогоняем каждый текст через модель doc2vec и формируем многомерный массив чисел\n",
        "    return pd.DataFrame(vectors_ndarray, columns=columns)            # оборачиваем его в датафрейм для удобства\n",
        "\n",
        "\n",
        "X_train = transform_text_array_to_vector_dataframe(X_train_texts)    # наконец создадим датафреймы, которые сможем подать в модель классификации\n",
        "X_test = transform_text_array_to_vector_dataframe(X_test_texts)"
      ],
      "metadata": {
        "id": "JtHXANZiH3-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вы уже познакомились с некоторыми \"деревянными\" методами машинного обучения - решающим деревом и случайным лесом. Градиентный бустинг - это итеративный способ построения классификации, полученный (как и случайный лес) путем комбинации нескольких алгоритмов. Сначала строится обычное решающее дерево. Затем строится ряд моделей, предсказывающих ошибку исходной модели. Эти предсказания вычитаются из исходной модели. Таким образом, в итоге мы имеем один классификатор, но намного более точный, чем обычное решающее дерево. Чем больше итераций этого алгоритма будет проведено, тем выше получится качество модели, но она будет дольше обучаться.\n",
        "\n",
        "Градиентный бустинг над решающими деревьями - это, пожалуй, самая распространенная на сегодняшний день модель машинного обучения. Ее используют для решениях многих задач, начиная кредитным скорингом и заканчивая товарным спросом и антифродом. Мы будем использовать эту модель для классификации наших текстов.\n",
        "\n",
        "Модель градиентного бустинга есть в библиотеке sklearn, но на больших данных она будет обучаться долго. На рынке сейчас популярны несколько оптимизированных реализаций градиентного бустинга. Самые известные - xgboost, lightgbm и catboost. Рассмотрим xgboost, но вы можете использовать любую из этих трех."
      ],
      "metadata": {
        "id": "nzBBKDL6Gjtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# установим библиотеку\n",
        "!pip install xgboost"
      ],
      "metadata": {
        "id": "3LZxnGLQGEUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Работать с моделью xgboost можно так же, как с моделями sklearn: fit и predict. Основные гиперпараметры - максимальная глубина деревьев модели и количество деревьев."
      ],
      "metadata": {
        "id": "wwY04bh6PP9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "xgb = XGBClassifier(max_depth=10, n_estimators=50)\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred = xgb.predict(X_test)\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "metadata": {
        "id": "9nTvmczdKg95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 2\n",
        "\n",
        "В этом задании от вас требуется провести классификацию текстов с использованием градиентного бустинга. Постройте такую модель, которая даст наилучший результат по метрике precision к классу 0, подобрав гиперпараметры:\n",
        "- минимальная встречаемость слова в текстах в doc2vec\n",
        "- максимальная глубина деревеьев в бустинге\n",
        "- количество деревьев в бустинге\n",
        "\n",
        "Дайте ответ на вопрос: лучше использовать более глубокие или более мелкие деревья в модели градиентного бустинга?"
      ],
      "metadata": {
        "id": "uZcI-pQiPuPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Задания №2\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install xgboost\n",
        "\n",
        "tweets = [TaggedDocument(doc, [i]) for i, doc in enumerate(preprocessed_df.text)]\n",
        "\n",
        "# Разобьем набор текстов на тренировочную и тестовую выборки\n",
        "X_train_texts, X_test_texts, y_train, y_test = train_test_split(preprocessed_df.text, preprocessed_df.positive, test_size=0.2, random_state=21)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wLsUThFwPAtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создадим модель Doc2Vec с минимальной встречаемостью слова 2\n",
        "d2v = Doc2Vec(tweets, min_count=2)\n",
        "d2v.train(tweets, total_examples=len(tweets), epochs=20)  # подберем веса коэффициентов внутри модели, которые больше будут подходить к нашему набору текстов"
      ],
      "metadata": {
        "id": "HiQ3fAtrhVVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_text_array_to_vector_dataframe(text_array):\n",
        "    \"\"\"Функция, которая преобразует одномерную колонку списков слов из текстов\n",
        "    в датафрейм со значениями векторов этих текстов\"\"\"\n",
        "    columns = [str(n) for n in range(d2v.vector_size)]               # задаем список названий колонок - просто порядковые номера\n",
        "    vectors_ndarray = text_array.apply(d2v.infer_vector).to_list()  # прогоняем каждый текст через модель doc2vec и формируем многомерный массив чисел\n",
        "    return pd.DataFrame(vectors_ndarray, columns=columns)            # оборачиваем его в датафрейм для удобства\n",
        "\n",
        "\n",
        "X_train = transform_text_array_to_vector_dataframe(X_train_texts)    # наконец создадим датафреймы, которые сможем подать в модель классификации\n",
        "X_test = transform_text_array_to_vector_dataframe(X_test_texts)"
      ],
      "metadata": {
        "id": "aZYWzJrnhXZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Применим модель xgboost. Основные гиперпараметры: max_depth - максимальная глубина деревьев модели и n_estimators - количество деревьев.\n",
        "# min_count=2, epochs=20\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "xgb = XGBClassifier(max_depth=10, n_estimators=50)\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred = xgb.predict(X_test)\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "metadata": {
        "id": "Cei8IEHohZtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Минимальная встречаемость 10\n",
        "# min_count=10, epochs=20\n",
        "\n",
        "d2v = Doc2Vec(tweets, min_count=10)\n",
        "d2v.train(tweets, total_examples=len(tweets), epochs=20)\n",
        "\n",
        "X_train10 = transform_text_array_to_vector_dataframe(X_train_texts)\n",
        "X_test10 = transform_text_array_to_vector_dataframe(X_test_texts)\n",
        "\n",
        "xgb.fit(X_train10, y_train)\n",
        "y_pred = xgb.predict(X_test10)\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "metadata": {
        "id": "_GG_ozFEhbvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Минимальная встречаемость 100\n",
        "# min_count=100, epochs=20\n",
        "\n",
        "d2v = Doc2Vec(tweets, min_count=100)\n",
        "d2v.train(tweets, total_examples=len(tweets), epochs=20)\n",
        "\n",
        "X_train100 = transform_text_array_to_vector_dataframe(X_train_texts)\n",
        "X_test100 = transform_text_array_to_vector_dataframe(X_test_texts)\n",
        "\n",
        "xgb.fit(X_train100, y_train)\n",
        "y_pred = xgb.predict(X_test100)\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "metadata": {
        "id": "5TrdQ4dthdzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Применим параметры, общие для всех min_count=10, epochs=10\n",
        "\n",
        "d2v = Doc2Vec(tweets, min_count=10)\n",
        "d2v.train(tweets, total_examples=len(tweets), epochs=10)\n",
        "\n",
        "X_train = transform_text_array_to_vector_dataframe(X_train_texts)\n",
        "X_test = transform_text_array_to_vector_dataframe(X_test_texts)"
      ],
      "metadata": {
        "id": "3hgs_vNWhf2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier(max_depth=5, n_estimators=50)\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred = xgb.predict(X_test)\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "metadata": {
        "id": "Ya7YqVLHhgiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier(max_depth=10, n_estimators=50)\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred = xgb.predict(X_test)\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "metadata": {
        "id": "VjJPUTwEhikS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier(max_depth=10, n_estimators=10)\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred = xgb.predict(X_test)\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "metadata": {
        "id": "UUDcSe1UhlSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier(max_depth=10, n_estimators=100)\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred = xgb.predict(X_test)\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "metadata": {
        "id": "c4D5zPsGhlx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод.\n",
        "Лучше использовать более глубокие деревья в модели градиентного бустинга. Увеличение количества деревьев также улучшает результат"
      ],
      "metadata": {
        "id": "Z2gbeOhchnXf"
      }
    }
  ]
}